{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "euro_en = load_db('pt-en/europarl-v7.pt-en.en')\n",
    "euro_pt = load_db('pt-en/europarl-v7.pt-en.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en sample example:  Resumption of the session\n",
      "pt sample example:  Reinício da sessão\n"
     ]
    }
   ],
   "source": [
    "print('en sample example: ', euro_en.split('\\n')[0])\n",
    "print('pt sample example: ', euro_pt.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data en:  Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\n",
      "Data pt:  Será que a senhora Presidente poderia enviar uma carta à Presidente do Sri Lanka manifestando o pesar do Parlamento por esta e outras mortes violentas perpetradas no seu país, e instando­a a envidar todos os esforços ao seu alcance para procurar obter uma reconciliação pacífica na situação extremamente difícil que ali se vive?\n"
     ]
    }
   ],
   "source": [
    "def data_cleaning(data):\n",
    "    data = re.sub(r'\\.(?=[0-9]|[a-z]|[A-Z])', '$$', data)\n",
    "    data = re.sub(r'\\$\\$', '', data)\n",
    "    data = re.sub(r' +', ' ', data)\n",
    "    return data.split('\\n')\n",
    "\n",
    "data_en = data_cleaning(data=euro_en)\n",
    "data_pt = data_cleaning(data=euro_pt)\n",
    "\n",
    "print('Data en: ', data_en[10])\n",
    "print('Data pt: ', data_pt[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en data size: 1960408 | pt data size: 1960408\n"
     ]
    }
   ],
   "source": [
    "print('en data size: {} | pt data size: {}'.format(len(data_en), len(data_pt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En vocab size:  8191\n",
      "Pt vocab size:  8116\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_data(data, vocab_size=2**13):\n",
    "    return tfds.features.text.SubwordTextEncoder.build_from_corpus(data, target_vocab_size=vocab_size)\n",
    "\n",
    "tokenizer_en = tokenizer_data(data=data_en)\n",
    "tokenizer_pt = tokenizer_data(data=data_pt)\n",
    "\n",
    "print('En vocab size: ', tokenizer_en.vocab_size)\n",
    "print('Pt vocab size: ', tokenizer_pt.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8191, 2458, 972, 2108, 3, 1, 2571, 8192]\n",
      "[8191, 11, 5645, 7093, 1, 2634, 3, 1, 23, 67, 2144, 4821, 50, 12, 6727, 7967, 3951, 1446, 1974, 2, 5, 11, 33, 57, 414, 397, 4, 311, 55, 7, 1684, 79, 323, 6, 1, 210, 8, 55, 4288, 43, 7, 5242, 1654, 7697, 742, 1765, 7981, 8192]\n",
      "[8191, 944, 2008, 2, 21, 55, 26, 18, 1436, 8045, 2, 1, 3263, 3127, 8035, 186, 1756, 2997, 7472, 493, 1691, 8038, 90, 2222, 4, 4498, 3815, 2, 175, 1, 102, 6, 7, 214, 3, 103, 4142, 7, 2174, 3, 1035, 4956, 8, 1995, 117, 3263, 2144, 2230, 7981, 8192]\n",
      "[8191, 554, 18, 3508, 7, 179, 12, 16, 435, 6, 1, 454, 3, 1, 274, 319, 3313, 2, 353, 16, 679, 7980, 2571, 7981, 8192]\n",
      "[8191, 62, 1, 7496, 2, 11, 35, 57, 4, 7157, 7, 4732, 90, 13, 7835, 8036, 2, 21, 7, 214, 3, 321, 18, 2628, 93, 2, 12, 264, 3, 40, 1, 1275, 587, 2, 235, 99, 3, 1, 5218, 2261, 3500, 2, 6, 1, 417, 103, 3, 1, 23, 113, 7981, 8192]\n",
      "[8116, 834, 705, 7, 3561, 8117]\n",
      "[8116, 4808, 1981, 6, 7004, 2487, 3, 3, 1192, 8, 50, 236, 1, 4, 1109, 218, 5803, 3997, 1152, 22, 7016, 7957, 7905, 1875, 1, 6088, 2, 1877, 702, 1, 5, 1434, 2960, 6, 59, 9, 584, 2353, 7975, 1, 2222, 81, 4, 793, 2200, 1819, 6615, 122, 7906, 8117]\n",
      "[8116, 322, 5669, 82, 3667, 7974, 1, 6, 6771, 152, 1420, 2461, 8, 338, 1112, 377, 17, 3521, 7977, 121, 97, 5814, 1, 9, 165, 2, 194, 15, 166, 62, 167, 1126, 2, 4822, 5797, 1430, 5057, 135, 7906, 8117]\n",
      "[8116, 138, 691, 7560, 82, 6, 1022, 2, 16, 1469, 3, 14, 196, 33, 6, 759, 34, 1219, 2689, 1, 290, 46, 409, 2, 7854, 303, 7906, 8117]\n",
      "[8116, 5876, 1, 5003, 42, 27, 37, 70, 61, 929, 25, 14, 747, 308, 2, 613, 42, 4, 6820, 1617, 2682, 14, 4769, 240, 2, 2334, 3458, 3661, 25, 173, 12, 6992, 1, 478, 23, 1036, 5205, 1122, 7975, 1, 34, 473, 62, 7, 40, 67, 4, 167, 4919, 79, 7906, 8117]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_start_end(data, tokenizer):\n",
    "    vocab_size = tokenizer.vocab_size + 2\n",
    "    # adding start and end token in each setense\n",
    "    return [[vocab_size - 2] + tokenizer.encode(sentense) + [vocab_size - 1] for sentense in data]\n",
    "\n",
    "inputs = token_start_end(data=data_en, tokenizer=tokenizer_en)\n",
    "outputs = token_start_end(data=data_pt, tokenizer=tokenizer_pt)\n",
    "\n",
    "[print(inputs[i] )for i in range(5)]\n",
    "[print(outputs[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing setense longer than 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "66118it [00:09, 6859.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len inputs: 208990 | len outputs 208990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_longer_sentense(data, max_length=15):\n",
    "    idx_to_remove = [idx for idx, sentense in enumerate(data) if len(sentense) > max_length]\n",
    "\n",
    "    for idx in tqdm(reversed(idx_to_remove)):\n",
    "        # remove the same setense from the data\n",
    "        del inputs[idx]\n",
    "        del outputs[idx]\n",
    "\n",
    "remove_longer_sentense(data=inputs)\n",
    "remove_longer_sentense(data=outputs)\n",
    "\n",
    "print('len inputs: {} | len outputs {}'.format(len(inputs), len(outputs)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cbe59bc8e43c33e263d0ddce44ad7ec5ba3f57d4d2daa17d616eb06d4130f325"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
