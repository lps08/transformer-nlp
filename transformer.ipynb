{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "euro_en = load_db('pt-en/europarl-v7.pt-en.en')\n",
    "euro_pt = load_db('pt-en/europarl-v7.pt-en.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en sample example:  Resumption of the session\n",
      "pt sample example:  Reinício da sessão\n"
     ]
    }
   ],
   "source": [
    "print('en sample example: ', euro_en.split('\\n')[0])\n",
    "print('pt sample example: ', euro_pt.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data en:  Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\n",
      "Data pt:  Será que a senhora Presidente poderia enviar uma carta à Presidente do Sri Lanka manifestando o pesar do Parlamento por esta e outras mortes violentas perpetradas no seu país, e instando­a a envidar todos os esforços ao seu alcance para procurar obter uma reconciliação pacífica na situação extremamente difícil que ali se vive?\n"
     ]
    }
   ],
   "source": [
    "def data_cleaning(data):\n",
    "    data = re.sub(r'\\.(?=[0-9]|[a-z]|[A-Z])', '$$', data)\n",
    "    data = re.sub(r'\\$\\$', '', data)\n",
    "    data = re.sub(r' +', ' ', data)\n",
    "    return data.split('\\n')\n",
    "\n",
    "data_en = data_cleaning(data=euro_en)\n",
    "data_pt = data_cleaning(data=euro_pt)\n",
    "\n",
    "print('Data en: ', data_en[10])\n",
    "print('Data pt: ', data_pt[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en data size: 1960408 | pt data size: 1960408\n"
     ]
    }
   ],
   "source": [
    "print('en data size: {} | pt data size: {}'.format(len(data_en), len(data_pt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En vocab size:  8191\n",
      "Pt vocab size:  8116\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_data(data, vocab_size=2**13):\n",
    "    return tfds.features.text.SubwordTextEncoder.build_from_corpus(data, target_vocab_size=vocab_size)\n",
    "\n",
    "tokenizer_en = tokenizer_data(data=data_en)\n",
    "tokenizer_pt = tokenizer_data(data=data_pt)\n",
    "\n",
    "print('En vocab size: ', tokenizer_en.vocab_size)\n",
    "print('Pt vocab size: ', tokenizer_pt.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input example:  [8191, 2458, 972, 2108, 3, 1, 2571, 8192]\n",
      "Output example:  [8116, 834, 705, 7, 3561, 8117]\n"
     ]
    }
   ],
   "source": [
    "def token_start_end(data, tokenizer):\n",
    "    vocab_size = tokenizer.vocab_size + 2\n",
    "    # adding start and end token in each setense\n",
    "    return [[vocab_size - 2] + tokenizer.encode(sentense) + [vocab_size - 1] for sentense in data]\n",
    "\n",
    "inputs = token_start_end(data=data_en, tokenizer=tokenizer_en)\n",
    "outputs = token_start_end(data=data_pt, tokenizer=tokenizer_pt)\n",
    "\n",
    "print('Input example: ', inputs[0])\n",
    "print('Output example: ', outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing setense longer than 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1685300it [06:45, 4152.61it/s] \n",
      "66118it [00:09, 6720.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len inputs: 208990 | len outputs 208990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_longer_sentense(data, max_length=15):\n",
    "    idx_to_remove = [idx for idx, sentense in enumerate(data) if len(sentense) > max_length]\n",
    "\n",
    "    for idx in tqdm(reversed(idx_to_remove)):\n",
    "        # remove the same setense from the data\n",
    "        del inputs[idx]\n",
    "        del outputs[idx]\n",
    "\n",
    "remove_longer_sentense(data=inputs)\n",
    "remove_longer_sentense(data=outputs)\n",
    "\n",
    "print('len inputs: {} | len outputs {}'.format(len(inputs), len(outputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding sentenses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input padded sequences example:  [8191 2458  972 2108    3    1 2571 8192    0    0    0    0    0    0\n",
      "    0]\n",
      "Output padded sequences example:  [8116  834  705    7 3561 8117    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "def padding_sequences(data, max_length):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(sequences=data, value=0, padding='post', maxlen=max_length)\n",
    "\n",
    "inputs = padding_sequences(data=inputs, max_length=15)\n",
    "outputs = padding_sequences(data=outputs, max_length=15)\n",
    "\n",
    "print('Input padded sequences example: ', inputs[0])\n",
    "print('Output padded sequences example: ', outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating final dataset with tf optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensors=(inputs, outputs))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cbe59bc8e43c33e263d0ddce44ad7ec5ba3f57d4d2daa17d616eb06d4130f325"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
