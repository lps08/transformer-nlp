{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 12:06:20.207162: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "euro_en = load_db('pt-en/europarl-v7.pt-en.en')\n",
    "euro_pt = load_db('pt-en/europarl-v7.pt-en.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en sample example:  Resumption of the session\n",
      "pt sample example:  Reinício da sessão\n"
     ]
    }
   ],
   "source": [
    "print('en sample example: ', euro_en.split('\\n')[0])\n",
    "print('pt sample example: ', euro_pt.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data en:  Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\n",
      "Data pt:  Será que a senhora Presidente poderia enviar uma carta à Presidente do Sri Lanka manifestando o pesar do Parlamento por esta e outras mortes violentas perpetradas no seu país, e instando­a a envidar todos os esforços ao seu alcance para procurar obter uma reconciliação pacífica na situação extremamente difícil que ali se vive?\n"
     ]
    }
   ],
   "source": [
    "def data_cleaning(data):\n",
    "    data = re.sub(r'\\.(?=[0-9]|[a-z]|[A-Z])', '$$', data)\n",
    "    data = re.sub(r'\\$\\$', '', data)\n",
    "    data = re.sub(r' +', ' ', data)\n",
    "    return data.split('\\n')\n",
    "\n",
    "data_en = data_cleaning(data=euro_en)\n",
    "data_pt = data_cleaning(data=euro_pt)\n",
    "\n",
    "print('Data en: ', data_en[10])\n",
    "print('Data pt: ', data_pt[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en data size: 1960408 | pt data size: 1960408\n"
     ]
    }
   ],
   "source": [
    "print('en data size: {} | pt data size: {}'.format(len(data_en), len(data_pt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En vocab size:  8191\n",
      "Pt vocab size:  8116\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_data(data, vocab_size):\n",
    "    return tfds.features.text.SubwordTextEncoder.build_from_corpus(data, target_vocab_size=vocab_size)\n",
    "\n",
    "tokenizer_en = tokenizer_data(data=data_en, vocab_size=2**13) # en data\n",
    "tokenizer_pt = tokenizer_data(data=data_pt, vocab_size=2**13) # pt data\n",
    "\n",
    "print('En vocab size: ', tokenizer_en.vocab_size)\n",
    "print('Pt vocab size: ', tokenizer_pt.vocab_size)\n",
    "\n",
    "vocab_size_en = tokenizer_en.vocab_size + 2\n",
    "vocab_size_pt = tokenizer_pt.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8191"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input example:  [8191, 2458, 972, 2108, 3, 1, 2571, 8192]\n",
      "Output example:  [8116, 834, 705, 7, 3561, 8117]\n"
     ]
    }
   ],
   "source": [
    "def token_start_end(data, tokenizer):\n",
    "    vocab_size = tokenizer.vocab_size + 2\n",
    "    # adding start and end token in each setense\n",
    "    return [[vocab_size - 2] + tokenizer.encode(sentense) + [vocab_size - 1] for sentense in data]\n",
    "\n",
    "inputs = token_start_end(data=data_en, tokenizer=tokenizer_en)\n",
    "outputs = token_start_end(data=data_pt, tokenizer=tokenizer_pt)\n",
    "\n",
    "print('Input example: ', inputs[0])\n",
    "print('Output example: ', outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing setenses longer than 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1685300it [07:09, 3923.94it/s] \n",
      "66118it [00:10, 6589.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len inputs: 208990 | len outputs 208990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_longer_sentense(data, max_length=15):\n",
    "    idx_to_remove = [idx for idx, sentense in enumerate(data) if len(sentense) > max_length]\n",
    "\n",
    "    for idx in tqdm(reversed(idx_to_remove)):\n",
    "        # remove the same setense in english and portuguese dataset\n",
    "        del inputs[idx]\n",
    "        del outputs[idx]\n",
    "\n",
    "remove_longer_sentense(data=inputs)\n",
    "remove_longer_sentense(data=outputs)\n",
    "\n",
    "print('len inputs: {} | len outputs {}'.format(len(inputs), len(outputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding sentenses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input padded sequences example:  [8191 2458  972 2108    3    1 2571 8192    0    0    0    0    0    0\n",
      "    0]\n",
      "Output padded sequences example:  [8116  834  705    7 3561 8117    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "def padding_sequences(data, max_length):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(sequences=data, value=0, padding='post', maxlen=max_length)\n",
    "\n",
    "inputs = padding_sequences(data=inputs, max_length=15)\n",
    "outputs = padding_sequences(data=outputs, max_length=15)\n",
    "\n",
    "print('Input padded sequences example: ', inputs[0])\n",
    "print('Output padded sequences example: ', outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final dataset cration with tf optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 12:22:27.675068: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-12 12:22:27.683407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-12 12:22:27.715283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:27.716238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.8095GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2022-04-12 12:22:27.716279: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-12 12:22:27.779533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-12 12:22:27.779595: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-12 12:22:27.813926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-12 12:22:27.823564: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-12 12:22:27.887895: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-12 12:22:27.896112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-12 12:22:27.898599: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-12 12:22:27.898730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:27.898910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:27.899006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-12 12:22:27.899757: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-12 12:22:27.900122: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-12 12:22:27.900202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:27.900317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.8095GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2022-04-12 12:22:27.900332: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-12 12:22:27.900347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-12 12:22:27.900356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-12 12:22:27.900365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-12 12:22:27.900373: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-12 12:22:27.900383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-12 12:22:27.900391: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-12 12:22:27.900399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-12 12:22:27.900447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:27.900582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:27.900672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-12 12:22:27.900694: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-12 12:22:28.126779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-12 12:22:28.126803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-04-12 12:22:28.126809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-04-12 12:22:28.126985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:28.127109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:28.127203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 12:22:28.127282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4870 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:05:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensors=(inputs, outputs))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pe](imgs/pe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ PE(pos, 2i) = sin(pos * angles)$\n",
    "\n",
    "$ PE(pos, 2i + 1) = cos(pos * angles)$\n",
    "\n",
    "\n",
    "$ angles = \\frac{1}{10000^{2*i / dmodel}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def get_angles(self, i, d_model):\n",
    "        # definition of angles\n",
    "        return 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "\n",
    "        # get the pos and i matrix to gerate positional encoding from a input tensor\n",
    "        pos = np.arange(seq_length)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "\n",
    "        # calculates the angle results\n",
    "        angles = self.get_angles(i=i, d_model=d_model)\n",
    "\n",
    "        # calculates the positional encoding\n",
    "        pe = pos * angles #(seq_length, d_model)\n",
    "        pe[:, 0::2] = np.sin(pe[:, 0::2]) #even position\n",
    "        pe[:, 1::2] = np.cos(pe[:, 1::2]) #odd position\n",
    "\n",
    "        # transform the pos encoding dimension to the same as the input\n",
    "        pos_encoding = pe[np.newaxis, ...]\n",
    "        pos_encoding = tf.cast(x=pos_encoding, dtype=tf.float32)\n",
    "\n",
    "        # print('Inputs shape: {} | PE shape {} | Pos encoding shape: {}'.format(inputs.shape, pe.shape, pos_encoding.shape))\n",
    "        \n",
    "        # print('Pos encoding', pos_encoding)\n",
    "\n",
    "        # print('Pos encoding shape {} | inputs shape {}'.format(pos_encoding.shape, inputs.shape))\n",
    "\n",
    "        return inputs + pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing positional encoding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs + pos encoding tf.Tensor(\n",
      "[[[1.        2.        1.        2.       ]\n",
      "  [1.841471  1.5403023 1.0099999 1.9999499]\n",
      "  [1.9092975 0.5838531 1.0199987 1.9998   ]\n",
      "  [1.14112   0.0100075 1.0299954 1.9995501]]], shape=(1, 4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pos = PositionalEncoding()\n",
    "\n",
    "matrix_test = tf.ones((1, 4, 4))\n",
    "\n",
    "print('Inputs + pos encoding', pos(matrix_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mecanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scaled dot product attention](imgs/scaled-dot-produt-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})*V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q = queries\n",
    "\n",
    "K = keys\n",
    "\n",
    "V = values\n",
    "\n",
    "$K^{T}$ = K matrix transpose\n",
    "\n",
    "$d_{k}$ = K dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "\n",
    "    scaled_produt = product / tf.math.sqrt(keys_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_produt += (mask * -1e9)\n",
    "\n",
    "    softmax = tf.nn.softmax(scaled_produt, axis=-1)\n",
    "\n",
    "    return tf.matmul(softmax, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mult-head attention](imgs/multi-head-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, num_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # projections\n",
    "        self.num_proj = num_proj\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = inputs comming from call method\n",
    "        # get the model shape\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        # print(self.d_model, self.num_proj)\n",
    "        \n",
    "        # splitting according to projections\n",
    "        assert self.d_model % self.num_proj == 0\n",
    "        self.d_proj = d_model // self.num_proj\n",
    "\n",
    "        # linear dense layers\n",
    "        self.query_linear_dense = layers.Dense(units = self.d_model)\n",
    "        self.keys_linear_dense = layers.Dense(units = self.d_model)\n",
    "        self.values_linear_dense = layers.Dense(units = self.d_model)\n",
    "\n",
    "        self.final_linear_dense = layers.Dense(units = self.d_model)\n",
    "\n",
    "    def split_proj(self, inputs, batch_size):\n",
    "        shape = (batch_size, -1, self.num_proj, self.d_proj)\n",
    "        splitted_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, num_proj, d_proj)\n",
    "\n",
    "        return tf.transpose(splitted_inputs, perm=[0, 2, 1, 3]) # (batch_size, num_proj, seq_length, d_proj)\n",
    "\n",
    "    def scaled_dot_product_attention(self, queries, keys, values, mask=None):\n",
    "        product = tf.matmul(queries, keys, transpose_b=True)\n",
    "        keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "\n",
    "        scaled_produt = product / tf.math.sqrt(keys_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_produt += (mask * -1e9)\n",
    "\n",
    "        softmax = tf.nn.softmax(scaled_produt, axis=-1)\n",
    "\n",
    "        return tf.matmul(softmax, values)\n",
    "\n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "\n",
    "        queries = self.query_linear_dense(queries)\n",
    "        keys = self.keys_linear_dense(keys)\n",
    "        values = self.values_linear_dense(values)\n",
    "\n",
    "        queries = self.split_proj(inputs=queries, batch_size=batch_size)\n",
    "        keys = self.split_proj(inputs=keys, batch_size=batch_size)\n",
    "        values = self.split_proj(inputs=values, batch_size=batch_size)\n",
    "\n",
    "        attention = self.scaled_dot_product_attention(queries = queries,\n",
    "                                                      keys = keys,\n",
    "                                                      values = values,\n",
    "                                                      mask=mask)\n",
    "\n",
    "        # return the same shape as the input\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
    "\n",
    "        return self.final_linear_dense(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encoder](imgs/encoder-layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, ff_units, num_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.ff_units = ff_units\n",
    "        self.num_proj = num_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(num_proj=self.num_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6) #1e-6 = 0.0000001\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=self.ff_units, activation='relu')\n",
    "        self.dense_2 = layers.Dense(units=self.d_model, activation='relu')\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, mask, training:bool):\n",
    "        attention = self.multi_head_attention(queries = inputs,\n",
    "                                              keys = inputs,\n",
    "                                              values = inputs,\n",
    "                                              mask = mask)\n",
    "        attention = self.dropout_1(attention, training = training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training = training)\n",
    "\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encoder](imgs/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 num_layers,\n",
    "                 ff_units, \n",
    "                 num_proj, \n",
    "                 dropout_rate, \n",
    "                 vocab_size, \n",
    "                 d_model, \n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.encoder_layers = [EncoderLayer(ff_units=ff_units, num_proj=num_proj, dropout_rate=dropout_rate) for _ in range(self.num_layers)]\n",
    "\n",
    "    def call(self, inputs, mask, training):\n",
    "        # embeddings        \n",
    "        embedding_inputs = self.embedding(inputs)\n",
    "        embedding_inputs *= tf.math.sqrt(x=tf.cast(x=self.d_model, dtype=tf.float32))\n",
    "\n",
    "        #positional encodings\n",
    "        pe = self.positional_encoding(inputs=embedding_inputs)\n",
    "        pe = self.dropout(pe, training=training)\n",
    "\n",
    "        # encoder layers\n",
    "        # enc_outputs = [enc_layer(inputs=pe, mask=mask, training=training) for enc_layer in self.encoder_layers]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            enc_outputs = self.encoder_layers[i](pe, mask, training)\n",
    "\n",
    "        return enc_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![decoder-layer](imgs/decoder-layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 ff_units, \n",
    "                 num_proj, \n",
    "                 dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.ff_units = ff_units\n",
    "        self.num_proj = num_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        # attention layer\n",
    "        self.mult_head_attention_1 = MultiHeadAttention(num_proj=self.num_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # attention layer\n",
    "        self.mult_head_attention_2 = MultiHeadAttention(num_proj=self.num_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # dense layer\n",
    "        self.dense_1 = layers.Dense(units=self.ff_units, activation='relu')\n",
    "        self.dense_2 = layers.Dense(units=self.d_model, activation='relu')\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        # attention mechanism block\n",
    "        attention_1 = self.mult_head_attention_1(queries = inputs, \n",
    "                                               keys = inputs, \n",
    "                                               values = inputs, \n",
    "                                               mask = mask_1)\n",
    "        attention_1 = self.dropout_1(attention_1, training = training)\n",
    "        attention_1 = self.norm_1(attention_1 + inputs)\n",
    "        \n",
    "        # attention mechanism block\n",
    "        attention_2 = self.mult_head_attention_2(queries = attention_1, \n",
    "                                               keys = enc_outputs, \n",
    "                                               values = enc_outputs, \n",
    "                                               mask = mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training = training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention_1)\n",
    "\n",
    "        # dense block\n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training = training)\n",
    "\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![decoder](imgs/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 ff_units, \n",
    "                 num_proj, \n",
    "                 dropout_rate, \n",
    "                 vocab_size, \n",
    "                 d_model, \n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "        self.decoder_layers = [DecoderLayer(ff_units=ff_units, num_proj=num_proj, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        # Embedding\n",
    "        embedding = self.embedding(inputs)\n",
    "        embedding *= tf.math.sqrt(x=tf.cast(x=self.d_model, dtype=tf.float32))\n",
    "\n",
    "        # Positional Encoding\n",
    "        pe = self.positional_encoding(inputs = embedding)\n",
    "        pe = self.dropout(pe, training)\n",
    "\n",
    "        # Decoder layers block\n",
    "        # dec_outputs = [dec_layer(inputs = pe, enc_outputs = enc_outputs, mask_1 = mask_1, mask_2 = mask_2, training = training) for dec_layer in self.decoder_layers]\n",
    "        for i in range(self.num_layers):\n",
    "            dec_outputs = self.decoder_layers[i](pe, enc_outputs, mask_1, mask_2, training)\n",
    "\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_padding_mask(seq): #(batch_size, seq_length) -> (batch_size, num_proj, seq_length, d_proj)\n",
    "        mask = tf.cast(x=tf.equal(seq, 0), dtype=tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(seq):\n",
    "    seq_len = tf.shape(seq)[-1]\n",
    "    \n",
    "    return 1 - tf.linalg.band_part(input=tf.ones(shape=(seq_len, seq_len)), num_lower=-1, num_upper=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_test = tf.cast(x=[[865, 526, 378, 128, 0, 0, 0]], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 7), dtype=float32, numpy=array([[[[0., 0., 0., 0., 1., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creating_padding_mask(seq=seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_look_ahead_mask(seq=seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 7, 7), dtype=float32, numpy=\n",
       "array([[[[0., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.maximum(x=creating_padding_mask(seq=seq_test), y=create_look_ahead_mask(seq=seq_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transformer](imgs/transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 vocab_en_size,\n",
    "                 vocab_pt_size,\n",
    "                 d_model,\n",
    "                 num_layers,\n",
    "                 ff_units,\n",
    "                 num_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"Transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               ff_units=ff_units,\n",
    "                               num_proj=num_proj,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               vocab_size=vocab_en_size,\n",
    "                               d_model=d_model)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers,\n",
    "                               ff_units=ff_units,\n",
    "                               num_proj=num_proj,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               vocab_size=vocab_pt_size,\n",
    "                               d_model=d_model)\n",
    "\n",
    "        self.output_dense = layers.Dense(units=vocab_pt_size, name=\"output_dense\")\n",
    "\n",
    "    def create_padding_mask(self, seq): #(batch_size, seq_length) -> (batch_size, num_proj, seq_length, d_proj)\n",
    "        mask = tf.cast(x=tf.math.equal(seq, 0), dtype=tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[-1]        \n",
    "        return 1 - tf.linalg.band_part(input=tf.ones(shape=(seq_len, seq_len)), num_lower=-1, num_upper=0)\n",
    "\n",
    "    def call(self, enc_inputs, dec_inputs, training:bool):\n",
    "        # masks\n",
    "        enc_mask = self.create_padding_mask(seq=enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(x=self.create_padding_mask(dec_inputs), y=self.create_look_ahead_mask(dec_inputs))\n",
    "        dec_mask_2 = self.create_padding_mask(seq=enc_inputs)\n",
    "\n",
    "        enc_outputs = self.encoder(inputs = enc_inputs, mask = enc_mask, training = training)\n",
    "        dec_outputs = self.decoder(inputs = dec_inputs, enc_outputs = enc_outputs, mask_1 = dec_mask_1, mask_2 = dec_mask_2, training = training)\n",
    "\n",
    "        outputs = self.output_dense(dec_outputs)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "d_model = 128 # recomends 512\n",
    "num_layers = 4 # recomends 6\n",
    "ff_units = 512 # recomends 2048\n",
    "num_proj = 8 # recomends 8\n",
    "dropout_rate = 0.1 # recomends 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(vocab_en_size=vocab_size_en,\n",
    "                         vocab_pt_size=vocab_size_pt,\n",
    "                         d_model=d_model,\n",
    "                         num_layers=num_layers,\n",
    "                         ff_units=ff_units,\n",
    "                         num_proj=num_proj,\n",
    "                         dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ob = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(x=target, y=0))\n",
    "    loss_ = loss_ob(y_true=target, y_pred=pred)\n",
    "\n",
    "    mask = tf.cast(x=mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(input_tensor=loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its recommended use Adam Optimize with $\\beta_{1} = 0.9$, $\\beta_{2} = 0.98$, $\\epsilon = 10^{-9}$ and custom learning rate schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$lrate = d\\_model^{-0.5} * min(step\\_num^{-0.5}, step\\_num * warmup\\_step^{-1.5})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{-0.5}$ = inverse square root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps = 4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(x=d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(x=step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(x=self.d_model) * tf.math.minimum(x=arg1, y=arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate = CustomSchedule(d_model=d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lrate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"checkpoint\"\n",
    "ckpt = tf.train.Checkpoint(transformer = transformer, optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint=ckpt, directory=checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Checkpoint restored!')\n",
    "\n",
    "else:\n",
    "    print('No checkpoint found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 12:22:29.259865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 0 | Accuracy 0.0000 | Loss 6.3823\n",
      "Epoch 1 | Batch 50 | Accuracy 0.0000 | Loss 6.4801\n",
      "Epoch 1 | Batch 100 | Accuracy 0.0158 | Loss 6.3840\n",
      "Epoch 1 | Batch 150 | Accuracy 0.0347 | Loss 6.2942\n",
      "Epoch 1 | Batch 200 | Accuracy 0.0446 | Loss 6.1965\n",
      "Epoch 1 | Batch 250 | Accuracy 0.0505 | Loss 6.0882\n",
      "Epoch 1 | Batch 300 | Accuracy 0.0545 | Loss 5.9648\n",
      "Epoch 1 | Batch 350 | Accuracy 0.0586 | Loss 5.8316\n",
      "Epoch 1 | Batch 400 | Accuracy 0.0654 | Loss 5.6932\n",
      "Epoch 1 | Batch 450 | Accuracy 0.0721 | Loss 5.5632\n",
      "Epoch 1 | Batch 500 | Accuracy 0.0781 | Loss 5.4458\n",
      "Epoch 1 | Batch 550 | Accuracy 0.0835 | Loss 5.3370\n",
      "Epoch 1 | Batch 600 | Accuracy 0.0888 | Loss 5.2266\n",
      "Epoch 1 | Batch 650 | Accuracy 0.0946 | Loss 5.1287\n",
      "Epoch 1 | Batch 700 | Accuracy 0.1007 | Loss 5.0322\n",
      "Epoch 1 | Batch 750 | Accuracy 0.1070 | Loss 4.9412\n",
      "Epoch 1 | Batch 800 | Accuracy 0.1129 | Loss 4.8555\n",
      "Epoch 1 | Batch 850 | Accuracy 0.1187 | Loss 4.7731\n",
      "Epoch 1 | Batch 900 | Accuracy 0.1242 | Loss 4.6969\n",
      "Epoch 1 | Batch 950 | Accuracy 0.1294 | Loss 4.6234\n",
      "Epoch 1 | Batch 1000 | Accuracy 0.1342 | Loss 4.5535\n",
      "Epoch 1 | Batch 1050 | Accuracy 0.1389 | Loss 4.4866\n",
      "Epoch 1 | Batch 1100 | Accuracy 0.1432 | Loss 4.4209\n",
      "Epoch 1 | Batch 1150 | Accuracy 0.1476 | Loss 4.3588\n",
      "Epoch 1 | Batch 1200 | Accuracy 0.1522 | Loss 4.2975\n",
      "Epoch 1 | Batch 1250 | Accuracy 0.1566 | Loss 4.2386\n",
      "Epoch 1 | Batch 1300 | Accuracy 0.1610 | Loss 4.1813\n",
      "Epoch 1 | Batch 1350 | Accuracy 0.1654 | Loss 4.1268\n",
      "Epoch 1 | Batch 1400 | Accuracy 0.1698 | Loss 4.0755\n",
      "Epoch 1 | Batch 1450 | Accuracy 0.1741 | Loss 4.0237\n",
      "Epoch 1 | Batch 1500 | Accuracy 0.1783 | Loss 3.9736\n",
      "Epoch 1 | Batch 1550 | Accuracy 0.1824 | Loss 3.9264\n",
      "Epoch 1 | Batch 1600 | Accuracy 0.1864 | Loss 3.8817\n",
      "Epoch 1 | Batch 1650 | Accuracy 0.1905 | Loss 3.8365\n",
      "Epoch 1 | Batch 1700 | Accuracy 0.1944 | Loss 3.7931\n",
      "Epoch 1 | Batch 1750 | Accuracy 0.1982 | Loss 3.7513\n",
      "Epoch 1 | Batch 1800 | Accuracy 0.2021 | Loss 3.7110\n",
      "Epoch 1 | Batch 1850 | Accuracy 0.2059 | Loss 3.6733\n",
      "Epoch 1 | Batch 1900 | Accuracy 0.2097 | Loss 3.6353\n",
      "Epoch 1 | Batch 1950 | Accuracy 0.2133 | Loss 3.6000\n",
      "Epoch 1 | Batch 2000 | Accuracy 0.2169 | Loss 3.5658\n",
      "Epoch 1 | Batch 2050 | Accuracy 0.2203 | Loss 3.5307\n",
      "Epoch 1 | Batch 2100 | Accuracy 0.2236 | Loss 3.4975\n",
      "Epoch 1 | Batch 2150 | Accuracy 0.2270 | Loss 3.4653\n",
      "Epoch 1 | Batch 2200 | Accuracy 0.2301 | Loss 3.4340\n",
      "Epoch 1 | Batch 2250 | Accuracy 0.2333 | Loss 3.4026\n",
      "Epoch 1 | Batch 2300 | Accuracy 0.2362 | Loss 3.3752\n",
      "Epoch 1 | Batch 2350 | Accuracy 0.2389 | Loss 3.3481\n",
      "Epoch 1 | Batch 2400 | Accuracy 0.2415 | Loss 3.3227\n",
      "Epoch 1 | Batch 2450 | Accuracy 0.2440 | Loss 3.2986\n",
      "Epoch 1 | Batch 2500 | Accuracy 0.2465 | Loss 3.2751\n",
      "Epoch 1 | Batch 2550 | Accuracy 0.2488 | Loss 3.2527\n",
      "Epoch 1 | Batch 2600 | Accuracy 0.2511 | Loss 3.2308\n",
      "Epoch 1 | Batch 2650 | Accuracy 0.2532 | Loss 3.2098\n",
      "Epoch 1 | Batch 2700 | Accuracy 0.2554 | Loss 3.1892\n",
      "Epoch 1 | Batch 2750 | Accuracy 0.2575 | Loss 3.1686\n",
      "Epoch 1 | Batch 2800 | Accuracy 0.2595 | Loss 3.1493\n",
      "Epoch 1 | Batch 2850 | Accuracy 0.2615 | Loss 3.1300\n",
      "Epoch 1 | Batch 2900 | Accuracy 0.2633 | Loss 3.1114\n",
      "Epoch 1 | Batch 2950 | Accuracy 0.2652 | Loss 3.0930\n",
      "Epoch 1 | Batch 3000 | Accuracy 0.2671 | Loss 3.0745\n",
      "Epoch 1 | Batch 3050 | Accuracy 0.2689 | Loss 3.0569\n",
      "Epoch 1 | Batch 3100 | Accuracy 0.2707 | Loss 3.0394\n",
      "Epoch 1 | Batch 3150 | Accuracy 0.2724 | Loss 3.0221\n",
      "Epoch 1 | Batch 3200 | Accuracy 0.2741 | Loss 3.0052\n",
      "Epoch 1 | Batch 3250 | Accuracy 0.2758 | Loss 2.9889\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 0: 320.95748376846313 secs \n",
      "\n",
      "Epoch 2 of 20\n",
      "Epoch 2 | Batch 0 | Accuracy 0.3862 | Loss 2.0503\n",
      "Epoch 2 | Batch 50 | Accuracy 0.3813 | Loss 2.0275\n",
      "Epoch 2 | Batch 100 | Accuracy 0.3833 | Loss 1.9886\n",
      "Epoch 2 | Batch 150 | Accuracy 0.3845 | Loss 1.9631\n",
      "Epoch 2 | Batch 200 | Accuracy 0.3855 | Loss 1.9430\n",
      "Epoch 2 | Batch 250 | Accuracy 0.3864 | Loss 1.9347\n",
      "Epoch 2 | Batch 300 | Accuracy 0.3872 | Loss 1.9184\n",
      "Epoch 2 | Batch 350 | Accuracy 0.3878 | Loss 1.9071\n",
      "Epoch 2 | Batch 400 | Accuracy 0.3885 | Loss 1.8898\n",
      "Epoch 2 | Batch 450 | Accuracy 0.3897 | Loss 1.8744\n",
      "Epoch 2 | Batch 500 | Accuracy 0.3908 | Loss 1.8630\n",
      "Epoch 2 | Batch 550 | Accuracy 0.3912 | Loss 1.8495\n",
      "Epoch 2 | Batch 600 | Accuracy 0.3914 | Loss 1.8417\n",
      "Epoch 2 | Batch 650 | Accuracy 0.3919 | Loss 1.8343\n",
      "Epoch 2 | Batch 700 | Accuracy 0.3920 | Loss 1.8285\n",
      "Epoch 2 | Batch 750 | Accuracy 0.3924 | Loss 1.8185\n",
      "Epoch 2 | Batch 800 | Accuracy 0.3929 | Loss 1.8129\n",
      "Epoch 2 | Batch 850 | Accuracy 0.3934 | Loss 1.8063\n",
      "Epoch 2 | Batch 900 | Accuracy 0.3936 | Loss 1.7977\n",
      "Epoch 2 | Batch 950 | Accuracy 0.3941 | Loss 1.7905\n",
      "Epoch 2 | Batch 1000 | Accuracy 0.3948 | Loss 1.7809\n",
      "Epoch 2 | Batch 1050 | Accuracy 0.3955 | Loss 1.7701\n",
      "Epoch 2 | Batch 1100 | Accuracy 0.3961 | Loss 1.7574\n",
      "Epoch 2 | Batch 1150 | Accuracy 0.3969 | Loss 1.7463\n",
      "Epoch 2 | Batch 1200 | Accuracy 0.3978 | Loss 1.7371\n",
      "Epoch 2 | Batch 1250 | Accuracy 0.3986 | Loss 1.7270\n",
      "Epoch 2 | Batch 1300 | Accuracy 0.3995 | Loss 1.7168\n",
      "Epoch 2 | Batch 1350 | Accuracy 0.4004 | Loss 1.7070\n",
      "Epoch 2 | Batch 1400 | Accuracy 0.4016 | Loss 1.6990\n",
      "Epoch 2 | Batch 1450 | Accuracy 0.4024 | Loss 1.6899\n",
      "Epoch 2 | Batch 1500 | Accuracy 0.4035 | Loss 1.6818\n",
      "Epoch 2 | Batch 1550 | Accuracy 0.4045 | Loss 1.6739\n",
      "Epoch 2 | Batch 1600 | Accuracy 0.4055 | Loss 1.6654\n",
      "Epoch 2 | Batch 1650 | Accuracy 0.4066 | Loss 1.6569\n",
      "Epoch 2 | Batch 1700 | Accuracy 0.4076 | Loss 1.6504\n",
      "Epoch 2 | Batch 1750 | Accuracy 0.4086 | Loss 1.6420\n",
      "Epoch 2 | Batch 1800 | Accuracy 0.4097 | Loss 1.6350\n",
      "Epoch 2 | Batch 1850 | Accuracy 0.4107 | Loss 1.6283\n",
      "Epoch 2 | Batch 1900 | Accuracy 0.4117 | Loss 1.6219\n",
      "Epoch 2 | Batch 1950 | Accuracy 0.4127 | Loss 1.6157\n",
      "Epoch 2 | Batch 2000 | Accuracy 0.4136 | Loss 1.6101\n",
      "Epoch 2 | Batch 2050 | Accuracy 0.4147 | Loss 1.6042\n",
      "Epoch 2 | Batch 2100 | Accuracy 0.4156 | Loss 1.5977\n",
      "Epoch 2 | Batch 2150 | Accuracy 0.4166 | Loss 1.5924\n",
      "Epoch 2 | Batch 2200 | Accuracy 0.4175 | Loss 1.5868\n",
      "Epoch 2 | Batch 2250 | Accuracy 0.4182 | Loss 1.5819\n",
      "Epoch 2 | Batch 2300 | Accuracy 0.4188 | Loss 1.5787\n",
      "Epoch 2 | Batch 2350 | Accuracy 0.4192 | Loss 1.5765\n",
      "Epoch 2 | Batch 2400 | Accuracy 0.4195 | Loss 1.5753\n",
      "Epoch 2 | Batch 2450 | Accuracy 0.4199 | Loss 1.5746\n",
      "Epoch 2 | Batch 2500 | Accuracy 0.4202 | Loss 1.5732\n",
      "Epoch 2 | Batch 2550 | Accuracy 0.4203 | Loss 1.5732\n",
      "Epoch 2 | Batch 2600 | Accuracy 0.4205 | Loss 1.5730\n",
      "Epoch 2 | Batch 2650 | Accuracy 0.4206 | Loss 1.5731\n",
      "Epoch 2 | Batch 2700 | Accuracy 0.4208 | Loss 1.5732\n",
      "Epoch 2 | Batch 2750 | Accuracy 0.4210 | Loss 1.5738\n",
      "Epoch 2 | Batch 2800 | Accuracy 0.4211 | Loss 1.5744\n",
      "Epoch 2 | Batch 2850 | Accuracy 0.4212 | Loss 1.5746\n",
      "Epoch 2 | Batch 2900 | Accuracy 0.4212 | Loss 1.5741\n",
      "Epoch 2 | Batch 2950 | Accuracy 0.4212 | Loss 1.5743\n",
      "Epoch 2 | Batch 3000 | Accuracy 0.4212 | Loss 1.5743\n",
      "Epoch 2 | Batch 3050 | Accuracy 0.4212 | Loss 1.5737\n",
      "Epoch 2 | Batch 3100 | Accuracy 0.4212 | Loss 1.5736\n",
      "Epoch 2 | Batch 3150 | Accuracy 0.4213 | Loss 1.5733\n",
      "Epoch 2 | Batch 3200 | Accuracy 0.4214 | Loss 1.5730\n",
      "Epoch 2 | Batch 3250 | Accuracy 0.4215 | Loss 1.5727\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 1: 329.40527606010437 secs \n",
      "\n",
      "Epoch 3 of 20\n",
      "Epoch 3 | Batch 0 | Accuracy 0.4308 | Loss 1.4901\n",
      "Epoch 3 | Batch 50 | Accuracy 0.4304 | Loss 1.5523\n",
      "Epoch 3 | Batch 100 | Accuracy 0.4322 | Loss 1.5454\n",
      "Epoch 3 | Batch 150 | Accuracy 0.4310 | Loss 1.5387\n",
      "Epoch 3 | Batch 200 | Accuracy 0.4302 | Loss 1.5342\n",
      "Epoch 3 | Batch 250 | Accuracy 0.4309 | Loss 1.5300\n",
      "Epoch 3 | Batch 300 | Accuracy 0.4315 | Loss 1.5233\n",
      "Epoch 3 | Batch 350 | Accuracy 0.4308 | Loss 1.5197\n",
      "Epoch 3 | Batch 400 | Accuracy 0.4310 | Loss 1.5092\n",
      "Epoch 3 | Batch 450 | Accuracy 0.4317 | Loss 1.4994\n",
      "Epoch 3 | Batch 500 | Accuracy 0.4325 | Loss 1.4936\n",
      "Epoch 3 | Batch 550 | Accuracy 0.4332 | Loss 1.4853\n",
      "Epoch 3 | Batch 600 | Accuracy 0.4331 | Loss 1.4782\n",
      "Epoch 3 | Batch 650 | Accuracy 0.4333 | Loss 1.4720\n",
      "Epoch 3 | Batch 700 | Accuracy 0.4341 | Loss 1.4667\n",
      "Epoch 3 | Batch 750 | Accuracy 0.4341 | Loss 1.4592\n",
      "Epoch 3 | Batch 800 | Accuracy 0.4346 | Loss 1.4538\n",
      "Epoch 3 | Batch 850 | Accuracy 0.4350 | Loss 1.4476\n",
      "Epoch 3 | Batch 900 | Accuracy 0.4353 | Loss 1.4415\n",
      "Epoch 3 | Batch 950 | Accuracy 0.4356 | Loss 1.4363\n",
      "Epoch 3 | Batch 1000 | Accuracy 0.4361 | Loss 1.4314\n",
      "Epoch 3 | Batch 1050 | Accuracy 0.4364 | Loss 1.4237\n",
      "Epoch 3 | Batch 1100 | Accuracy 0.4373 | Loss 1.4150\n",
      "Epoch 3 | Batch 1150 | Accuracy 0.4379 | Loss 1.4053\n",
      "Epoch 3 | Batch 1200 | Accuracy 0.4384 | Loss 1.3986\n",
      "Epoch 3 | Batch 1250 | Accuracy 0.4391 | Loss 1.3904\n",
      "Epoch 3 | Batch 1300 | Accuracy 0.4396 | Loss 1.3837\n",
      "Epoch 3 | Batch 1350 | Accuracy 0.4404 | Loss 1.3777\n",
      "Epoch 3 | Batch 1400 | Accuracy 0.4411 | Loss 1.3723\n",
      "Epoch 3 | Batch 1450 | Accuracy 0.4418 | Loss 1.3658\n",
      "Epoch 3 | Batch 1500 | Accuracy 0.4427 | Loss 1.3590\n",
      "Epoch 3 | Batch 1550 | Accuracy 0.4435 | Loss 1.3534\n",
      "Epoch 3 | Batch 1600 | Accuracy 0.4443 | Loss 1.3486\n",
      "Epoch 3 | Batch 1650 | Accuracy 0.4450 | Loss 1.3427\n",
      "Epoch 3 | Batch 1700 | Accuracy 0.4459 | Loss 1.3372\n",
      "Epoch 3 | Batch 1750 | Accuracy 0.4466 | Loss 1.3322\n",
      "Epoch 3 | Batch 1800 | Accuracy 0.4473 | Loss 1.3276\n",
      "Epoch 3 | Batch 1850 | Accuracy 0.4480 | Loss 1.3230\n",
      "Epoch 3 | Batch 1900 | Accuracy 0.4486 | Loss 1.3192\n",
      "Epoch 3 | Batch 1950 | Accuracy 0.4493 | Loss 1.3148\n",
      "Epoch 3 | Batch 2000 | Accuracy 0.4499 | Loss 1.3110\n",
      "Epoch 3 | Batch 2050 | Accuracy 0.4506 | Loss 1.3071\n",
      "Epoch 3 | Batch 2100 | Accuracy 0.4514 | Loss 1.3032\n",
      "Epoch 3 | Batch 2150 | Accuracy 0.4522 | Loss 1.2992\n",
      "Epoch 3 | Batch 2200 | Accuracy 0.4530 | Loss 1.2963\n",
      "Epoch 3 | Batch 2250 | Accuracy 0.4535 | Loss 1.2938\n",
      "Epoch 3 | Batch 2300 | Accuracy 0.4539 | Loss 1.2930\n",
      "Epoch 3 | Batch 2350 | Accuracy 0.4540 | Loss 1.2926\n",
      "Epoch 3 | Batch 2400 | Accuracy 0.4542 | Loss 1.2932\n",
      "Epoch 3 | Batch 2450 | Accuracy 0.4542 | Loss 1.2942\n",
      "Epoch 3 | Batch 2500 | Accuracy 0.4542 | Loss 1.2955\n",
      "Epoch 3 | Batch 2550 | Accuracy 0.4540 | Loss 1.2977\n",
      "Epoch 3 | Batch 2600 | Accuracy 0.4540 | Loss 1.2991\n",
      "Epoch 3 | Batch 2650 | Accuracy 0.4540 | Loss 1.3012\n",
      "Epoch 3 | Batch 2700 | Accuracy 0.4539 | Loss 1.3035\n",
      "Epoch 3 | Batch 2750 | Accuracy 0.4539 | Loss 1.3054\n",
      "Epoch 3 | Batch 2800 | Accuracy 0.4537 | Loss 1.3079\n",
      "Epoch 3 | Batch 2850 | Accuracy 0.4536 | Loss 1.3092\n",
      "Epoch 3 | Batch 2900 | Accuracy 0.4534 | Loss 1.3102\n",
      "Epoch 3 | Batch 2950 | Accuracy 0.4533 | Loss 1.3120\n",
      "Epoch 3 | Batch 3000 | Accuracy 0.4531 | Loss 1.3133\n",
      "Epoch 3 | Batch 3050 | Accuracy 0.4531 | Loss 1.3144\n",
      "Epoch 3 | Batch 3100 | Accuracy 0.4529 | Loss 1.3159\n",
      "Epoch 3 | Batch 3150 | Accuracy 0.4527 | Loss 1.3175\n",
      "Epoch 3 | Batch 3200 | Accuracy 0.4526 | Loss 1.3192\n",
      "Epoch 3 | Batch 3250 | Accuracy 0.4524 | Loss 1.3209\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 2: 331.7652499675751 secs \n",
      "\n",
      "Epoch 4 of 20\n",
      "Epoch 4 | Batch 0 | Accuracy 0.3683 | Loss 1.3836\n",
      "Epoch 4 | Batch 50 | Accuracy 0.4439 | Loss 1.4479\n",
      "Epoch 4 | Batch 100 | Accuracy 0.4458 | Loss 1.4184\n",
      "Epoch 4 | Batch 150 | Accuracy 0.4449 | Loss 1.4142\n",
      "Epoch 4 | Batch 200 | Accuracy 0.4456 | Loss 1.4095\n",
      "Epoch 4 | Batch 250 | Accuracy 0.4469 | Loss 1.4047\n",
      "Epoch 4 | Batch 300 | Accuracy 0.4475 | Loss 1.3977\n",
      "Epoch 4 | Batch 350 | Accuracy 0.4477 | Loss 1.3903\n",
      "Epoch 4 | Batch 400 | Accuracy 0.4483 | Loss 1.3805\n",
      "Epoch 4 | Batch 450 | Accuracy 0.4481 | Loss 1.3714\n",
      "Epoch 4 | Batch 500 | Accuracy 0.4487 | Loss 1.3657\n",
      "Epoch 4 | Batch 550 | Accuracy 0.4493 | Loss 1.3567\n",
      "Epoch 4 | Batch 600 | Accuracy 0.4499 | Loss 1.3478\n",
      "Epoch 4 | Batch 650 | Accuracy 0.4505 | Loss 1.3426\n",
      "Epoch 4 | Batch 700 | Accuracy 0.4513 | Loss 1.3367\n",
      "Epoch 4 | Batch 750 | Accuracy 0.4517 | Loss 1.3309\n",
      "Epoch 4 | Batch 800 | Accuracy 0.4523 | Loss 1.3229\n",
      "Epoch 4 | Batch 850 | Accuracy 0.4528 | Loss 1.3169\n",
      "Epoch 4 | Batch 900 | Accuracy 0.4529 | Loss 1.3120\n",
      "Epoch 4 | Batch 950 | Accuracy 0.4529 | Loss 1.3068\n",
      "Epoch 4 | Batch 1000 | Accuracy 0.4527 | Loss 1.3014\n",
      "Epoch 4 | Batch 1050 | Accuracy 0.4530 | Loss 1.2950\n",
      "Epoch 4 | Batch 1100 | Accuracy 0.4534 | Loss 1.2873\n",
      "Epoch 4 | Batch 1150 | Accuracy 0.4539 | Loss 1.2794\n",
      "Epoch 4 | Batch 1200 | Accuracy 0.4545 | Loss 1.2710\n",
      "Epoch 4 | Batch 1250 | Accuracy 0.4549 | Loss 1.2643\n",
      "Epoch 4 | Batch 1300 | Accuracy 0.4553 | Loss 1.2592\n",
      "Epoch 4 | Batch 1350 | Accuracy 0.4561 | Loss 1.2535\n",
      "Epoch 4 | Batch 1400 | Accuracy 0.4568 | Loss 1.2483\n",
      "Epoch 4 | Batch 1450 | Accuracy 0.4574 | Loss 1.2418\n",
      "Epoch 4 | Batch 1500 | Accuracy 0.4581 | Loss 1.2363\n",
      "Epoch 4 | Batch 1550 | Accuracy 0.4589 | Loss 1.2312\n",
      "Epoch 4 | Batch 1600 | Accuracy 0.4597 | Loss 1.2276\n",
      "Epoch 4 | Batch 1650 | Accuracy 0.4603 | Loss 1.2228\n",
      "Epoch 4 | Batch 1700 | Accuracy 0.4610 | Loss 1.2181\n",
      "Epoch 4 | Batch 1750 | Accuracy 0.4616 | Loss 1.2137\n",
      "Epoch 4 | Batch 1800 | Accuracy 0.4623 | Loss 1.2104\n",
      "Epoch 4 | Batch 1850 | Accuracy 0.4631 | Loss 1.2068\n",
      "Epoch 4 | Batch 1900 | Accuracy 0.4637 | Loss 1.2036\n",
      "Epoch 4 | Batch 1950 | Accuracy 0.4644 | Loss 1.2002\n",
      "Epoch 4 | Batch 2000 | Accuracy 0.4651 | Loss 1.1968\n",
      "Epoch 4 | Batch 2050 | Accuracy 0.4657 | Loss 1.1933\n",
      "Epoch 4 | Batch 2100 | Accuracy 0.4663 | Loss 1.1909\n",
      "Epoch 4 | Batch 2150 | Accuracy 0.4670 | Loss 1.1875\n",
      "Epoch 4 | Batch 2200 | Accuracy 0.4676 | Loss 1.1850\n",
      "Epoch 4 | Batch 2250 | Accuracy 0.4681 | Loss 1.1828\n",
      "Epoch 4 | Batch 2300 | Accuracy 0.4685 | Loss 1.1818\n",
      "Epoch 4 | Batch 2350 | Accuracy 0.4685 | Loss 1.1822\n",
      "Epoch 4 | Batch 2400 | Accuracy 0.4685 | Loss 1.1827\n",
      "Epoch 4 | Batch 2450 | Accuracy 0.4685 | Loss 1.1840\n",
      "Epoch 4 | Batch 2500 | Accuracy 0.4684 | Loss 1.1865\n",
      "Epoch 4 | Batch 2550 | Accuracy 0.4683 | Loss 1.1892\n",
      "Epoch 4 | Batch 2600 | Accuracy 0.4683 | Loss 1.1917\n",
      "Epoch 4 | Batch 2650 | Accuracy 0.4682 | Loss 1.1943\n",
      "Epoch 4 | Batch 2700 | Accuracy 0.4681 | Loss 1.1964\n",
      "Epoch 4 | Batch 2750 | Accuracy 0.4678 | Loss 1.1991\n",
      "Epoch 4 | Batch 2800 | Accuracy 0.4676 | Loss 1.2009\n",
      "Epoch 4 | Batch 2850 | Accuracy 0.4674 | Loss 1.2033\n",
      "Epoch 4 | Batch 2900 | Accuracy 0.4671 | Loss 1.2053\n",
      "Epoch 4 | Batch 2950 | Accuracy 0.4669 | Loss 1.2075\n",
      "Epoch 4 | Batch 3000 | Accuracy 0.4667 | Loss 1.2096\n",
      "Epoch 4 | Batch 3050 | Accuracy 0.4665 | Loss 1.2117\n",
      "Epoch 4 | Batch 3100 | Accuracy 0.4662 | Loss 1.2136\n",
      "Epoch 4 | Batch 3150 | Accuracy 0.4662 | Loss 1.2150\n",
      "Epoch 4 | Batch 3200 | Accuracy 0.4660 | Loss 1.2168\n",
      "Epoch 4 | Batch 3250 | Accuracy 0.4658 | Loss 1.2186\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 3: 333.84431195259094 secs \n",
      "\n",
      "Epoch 5 of 20\n",
      "Epoch 5 | Batch 0 | Accuracy 0.4230 | Loss 1.4010\n",
      "Epoch 5 | Batch 50 | Accuracy 0.4521 | Loss 1.3696\n",
      "Epoch 5 | Batch 100 | Accuracy 0.4531 | Loss 1.3618\n",
      "Epoch 5 | Batch 150 | Accuracy 0.4550 | Loss 1.3440\n",
      "Epoch 5 | Batch 200 | Accuracy 0.4550 | Loss 1.3313\n",
      "Epoch 5 | Batch 250 | Accuracy 0.4565 | Loss 1.3300\n",
      "Epoch 5 | Batch 300 | Accuracy 0.4570 | Loss 1.3225\n",
      "Epoch 5 | Batch 350 | Accuracy 0.4577 | Loss 1.3099\n",
      "Epoch 5 | Batch 400 | Accuracy 0.4584 | Loss 1.3016\n",
      "Epoch 5 | Batch 450 | Accuracy 0.4587 | Loss 1.2932\n",
      "Epoch 5 | Batch 500 | Accuracy 0.4594 | Loss 1.2836\n",
      "Epoch 5 | Batch 550 | Accuracy 0.4593 | Loss 1.2782\n",
      "Epoch 5 | Batch 600 | Accuracy 0.4594 | Loss 1.2739\n",
      "Epoch 5 | Batch 650 | Accuracy 0.4596 | Loss 1.2675\n",
      "Epoch 5 | Batch 700 | Accuracy 0.4600 | Loss 1.2616\n",
      "Epoch 5 | Batch 750 | Accuracy 0.4608 | Loss 1.2573\n",
      "Epoch 5 | Batch 800 | Accuracy 0.4613 | Loss 1.2518\n",
      "Epoch 5 | Batch 850 | Accuracy 0.4617 | Loss 1.2480\n",
      "Epoch 5 | Batch 900 | Accuracy 0.4614 | Loss 1.2444\n",
      "Epoch 5 | Batch 950 | Accuracy 0.4618 | Loss 1.2412\n",
      "Epoch 5 | Batch 1000 | Accuracy 0.4621 | Loss 1.2363\n",
      "Epoch 5 | Batch 1050 | Accuracy 0.4626 | Loss 1.2292\n",
      "Epoch 5 | Batch 1100 | Accuracy 0.4634 | Loss 1.2209\n",
      "Epoch 5 | Batch 1150 | Accuracy 0.4636 | Loss 1.2137\n",
      "Epoch 5 | Batch 1200 | Accuracy 0.4641 | Loss 1.2063\n",
      "Epoch 5 | Batch 1250 | Accuracy 0.4645 | Loss 1.2003\n",
      "Epoch 5 | Batch 1300 | Accuracy 0.4651 | Loss 1.1944\n",
      "Epoch 5 | Batch 1350 | Accuracy 0.4656 | Loss 1.1891\n",
      "Epoch 5 | Batch 1400 | Accuracy 0.4662 | Loss 1.1843\n",
      "Epoch 5 | Batch 1450 | Accuracy 0.4668 | Loss 1.1793\n",
      "Epoch 5 | Batch 1500 | Accuracy 0.4674 | Loss 1.1750\n",
      "Epoch 5 | Batch 1550 | Accuracy 0.4681 | Loss 1.1704\n",
      "Epoch 5 | Batch 1600 | Accuracy 0.4688 | Loss 1.1658\n",
      "Epoch 5 | Batch 1650 | Accuracy 0.4693 | Loss 1.1606\n",
      "Epoch 5 | Batch 1700 | Accuracy 0.4699 | Loss 1.1556\n",
      "Epoch 5 | Batch 1750 | Accuracy 0.4705 | Loss 1.1512\n",
      "Epoch 5 | Batch 1800 | Accuracy 0.4710 | Loss 1.1473\n",
      "Epoch 5 | Batch 1850 | Accuracy 0.4715 | Loss 1.1437\n",
      "Epoch 5 | Batch 1900 | Accuracy 0.4722 | Loss 1.1407\n",
      "Epoch 5 | Batch 1950 | Accuracy 0.4728 | Loss 1.1376\n",
      "Epoch 5 | Batch 2000 | Accuracy 0.4736 | Loss 1.1344\n",
      "Epoch 5 | Batch 2050 | Accuracy 0.4742 | Loss 1.1320\n",
      "Epoch 5 | Batch 2100 | Accuracy 0.4748 | Loss 1.1288\n",
      "Epoch 5 | Batch 2150 | Accuracy 0.4755 | Loss 1.1262\n",
      "Epoch 5 | Batch 2200 | Accuracy 0.4761 | Loss 1.1242\n",
      "Epoch 5 | Batch 2250 | Accuracy 0.4766 | Loss 1.1224\n",
      "Epoch 5 | Batch 2300 | Accuracy 0.4766 | Loss 1.1221\n",
      "Epoch 5 | Batch 2350 | Accuracy 0.4770 | Loss 1.1218\n",
      "Epoch 5 | Batch 2400 | Accuracy 0.4769 | Loss 1.1235\n",
      "Epoch 5 | Batch 2450 | Accuracy 0.4769 | Loss 1.1253\n",
      "Epoch 5 | Batch 2500 | Accuracy 0.4768 | Loss 1.1272\n",
      "Epoch 5 | Batch 2550 | Accuracy 0.4766 | Loss 1.1287\n",
      "Epoch 5 | Batch 2600 | Accuracy 0.4765 | Loss 1.1311\n",
      "Epoch 5 | Batch 2650 | Accuracy 0.4763 | Loss 1.1334\n",
      "Epoch 5 | Batch 2700 | Accuracy 0.4762 | Loss 1.1355\n",
      "Epoch 5 | Batch 2750 | Accuracy 0.4759 | Loss 1.1384\n",
      "Epoch 5 | Batch 2800 | Accuracy 0.4758 | Loss 1.1413\n",
      "Epoch 5 | Batch 2850 | Accuracy 0.4756 | Loss 1.1439\n",
      "Epoch 5 | Batch 2900 | Accuracy 0.4753 | Loss 1.1461\n",
      "Epoch 5 | Batch 2950 | Accuracy 0.4750 | Loss 1.1482\n",
      "Epoch 5 | Batch 3000 | Accuracy 0.4747 | Loss 1.1504\n",
      "Epoch 5 | Batch 3050 | Accuracy 0.4745 | Loss 1.1525\n",
      "Epoch 5 | Batch 3100 | Accuracy 0.4743 | Loss 1.1543\n",
      "Epoch 5 | Batch 3150 | Accuracy 0.4742 | Loss 1.1562\n",
      "Epoch 5 | Batch 3200 | Accuracy 0.4739 | Loss 1.1584\n",
      "Epoch 5 | Batch 3250 | Accuracy 0.4737 | Loss 1.1604\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 4: 329.11098885536194 secs \n",
      "\n",
      "Epoch 6 of 20\n",
      "Epoch 6 | Batch 0 | Accuracy 0.4833 | Loss 1.3585\n",
      "Epoch 6 | Batch 50 | Accuracy 0.4567 | Loss 1.2934\n",
      "Epoch 6 | Batch 100 | Accuracy 0.4590 | Loss 1.2940\n",
      "Epoch 6 | Batch 150 | Accuracy 0.4602 | Loss 1.2979\n",
      "Epoch 6 | Batch 200 | Accuracy 0.4610 | Loss 1.2933\n",
      "Epoch 6 | Batch 250 | Accuracy 0.4631 | Loss 1.2825\n",
      "Epoch 6 | Batch 300 | Accuracy 0.4645 | Loss 1.2755\n",
      "Epoch 6 | Batch 350 | Accuracy 0.4650 | Loss 1.2743\n",
      "Epoch 6 | Batch 400 | Accuracy 0.4652 | Loss 1.2634\n",
      "Epoch 6 | Batch 450 | Accuracy 0.4651 | Loss 1.2520\n",
      "Epoch 6 | Batch 500 | Accuracy 0.4655 | Loss 1.2435\n",
      "Epoch 6 | Batch 550 | Accuracy 0.4655 | Loss 1.2368\n",
      "Epoch 6 | Batch 600 | Accuracy 0.4661 | Loss 1.2302\n",
      "Epoch 6 | Batch 650 | Accuracy 0.4664 | Loss 1.2246\n",
      "Epoch 6 | Batch 700 | Accuracy 0.4664 | Loss 1.2170\n",
      "Epoch 6 | Batch 750 | Accuracy 0.4669 | Loss 1.2124\n",
      "Epoch 6 | Batch 800 | Accuracy 0.4674 | Loss 1.2071\n",
      "Epoch 6 | Batch 850 | Accuracy 0.4679 | Loss 1.2029\n",
      "Epoch 6 | Batch 900 | Accuracy 0.4682 | Loss 1.2002\n",
      "Epoch 6 | Batch 950 | Accuracy 0.4684 | Loss 1.1953\n",
      "Epoch 6 | Batch 1000 | Accuracy 0.4686 | Loss 1.1903\n",
      "Epoch 6 | Batch 1050 | Accuracy 0.4691 | Loss 1.1840\n",
      "Epoch 6 | Batch 1100 | Accuracy 0.4692 | Loss 1.1763\n",
      "Epoch 6 | Batch 1150 | Accuracy 0.4698 | Loss 1.1689\n",
      "Epoch 6 | Batch 1200 | Accuracy 0.4701 | Loss 1.1624\n",
      "Epoch 6 | Batch 1250 | Accuracy 0.4705 | Loss 1.1557\n",
      "Epoch 6 | Batch 1300 | Accuracy 0.4711 | Loss 1.1489\n",
      "Epoch 6 | Batch 1350 | Accuracy 0.4717 | Loss 1.1426\n",
      "Epoch 6 | Batch 1400 | Accuracy 0.4724 | Loss 1.1366\n",
      "Epoch 6 | Batch 1450 | Accuracy 0.4730 | Loss 1.1320\n",
      "Epoch 6 | Batch 1500 | Accuracy 0.4737 | Loss 1.1276\n",
      "Epoch 6 | Batch 1550 | Accuracy 0.4743 | Loss 1.1232\n",
      "Epoch 6 | Batch 1600 | Accuracy 0.4750 | Loss 1.1193\n",
      "Epoch 6 | Batch 1650 | Accuracy 0.4757 | Loss 1.1152\n",
      "Epoch 6 | Batch 1700 | Accuracy 0.4762 | Loss 1.1108\n",
      "Epoch 6 | Batch 1750 | Accuracy 0.4767 | Loss 1.1064\n",
      "Epoch 6 | Batch 1800 | Accuracy 0.4772 | Loss 1.1028\n",
      "Epoch 6 | Batch 1850 | Accuracy 0.4778 | Loss 1.1008\n",
      "Epoch 6 | Batch 1900 | Accuracy 0.4785 | Loss 1.0975\n",
      "Epoch 6 | Batch 1950 | Accuracy 0.4792 | Loss 1.0942\n",
      "Epoch 6 | Batch 2000 | Accuracy 0.4800 | Loss 1.0910\n",
      "Epoch 6 | Batch 2050 | Accuracy 0.4805 | Loss 1.0882\n",
      "Epoch 6 | Batch 2100 | Accuracy 0.4812 | Loss 1.0866\n",
      "Epoch 6 | Batch 2150 | Accuracy 0.4818 | Loss 1.0841\n",
      "Epoch 6 | Batch 2200 | Accuracy 0.4824 | Loss 1.0816\n",
      "Epoch 6 | Batch 2250 | Accuracy 0.4827 | Loss 1.0796\n",
      "Epoch 6 | Batch 2300 | Accuracy 0.4829 | Loss 1.0796\n",
      "Epoch 6 | Batch 2350 | Accuracy 0.4830 | Loss 1.0801\n",
      "Epoch 6 | Batch 2400 | Accuracy 0.4830 | Loss 1.0809\n",
      "Epoch 6 | Batch 2450 | Accuracy 0.4830 | Loss 1.0826\n",
      "Epoch 6 | Batch 2500 | Accuracy 0.4829 | Loss 1.0847\n",
      "Epoch 6 | Batch 2550 | Accuracy 0.4828 | Loss 1.0878\n",
      "Epoch 6 | Batch 2600 | Accuracy 0.4825 | Loss 1.0902\n",
      "Epoch 6 | Batch 2650 | Accuracy 0.4823 | Loss 1.0924\n",
      "Epoch 6 | Batch 2700 | Accuracy 0.4822 | Loss 1.0950\n",
      "Epoch 6 | Batch 2750 | Accuracy 0.4820 | Loss 1.0978\n",
      "Epoch 6 | Batch 2800 | Accuracy 0.4818 | Loss 1.1004\n",
      "Epoch 6 | Batch 2850 | Accuracy 0.4814 | Loss 1.1033\n",
      "Epoch 6 | Batch 2900 | Accuracy 0.4812 | Loss 1.1053\n",
      "Epoch 6 | Batch 2950 | Accuracy 0.4809 | Loss 1.1079\n",
      "Epoch 6 | Batch 3000 | Accuracy 0.4807 | Loss 1.1101\n",
      "Epoch 6 | Batch 3050 | Accuracy 0.4804 | Loss 1.1123\n",
      "Epoch 6 | Batch 3100 | Accuracy 0.4801 | Loss 1.1144\n",
      "Epoch 6 | Batch 3150 | Accuracy 0.4799 | Loss 1.1165\n",
      "Epoch 6 | Batch 3200 | Accuracy 0.4798 | Loss 1.1186\n",
      "Epoch 6 | Batch 3250 | Accuracy 0.4795 | Loss 1.1209\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 5: 325.5158121585846 secs \n",
      "\n",
      "Epoch 7 of 20\n",
      "Epoch 7 | Batch 0 | Accuracy 0.4230 | Loss 1.3975\n",
      "Epoch 7 | Batch 50 | Accuracy 0.4678 | Loss 1.2929\n",
      "Epoch 7 | Batch 100 | Accuracy 0.4679 | Loss 1.2670\n",
      "Epoch 7 | Batch 150 | Accuracy 0.4667 | Loss 1.2647\n",
      "Epoch 7 | Batch 200 | Accuracy 0.4681 | Loss 1.2458\n",
      "Epoch 7 | Batch 250 | Accuracy 0.4685 | Loss 1.2409\n",
      "Epoch 7 | Batch 300 | Accuracy 0.4691 | Loss 1.2298\n",
      "Epoch 7 | Batch 350 | Accuracy 0.4691 | Loss 1.2229\n",
      "Epoch 7 | Batch 400 | Accuracy 0.4697 | Loss 1.2186\n",
      "Epoch 7 | Batch 450 | Accuracy 0.4697 | Loss 1.2131\n",
      "Epoch 7 | Batch 500 | Accuracy 0.4702 | Loss 1.2086\n",
      "Epoch 7 | Batch 550 | Accuracy 0.4711 | Loss 1.1999\n",
      "Epoch 7 | Batch 600 | Accuracy 0.4708 | Loss 1.1930\n",
      "Epoch 7 | Batch 650 | Accuracy 0.4715 | Loss 1.1877\n",
      "Epoch 7 | Batch 700 | Accuracy 0.4718 | Loss 1.1844\n",
      "Epoch 7 | Batch 750 | Accuracy 0.4721 | Loss 1.1801\n",
      "Epoch 7 | Batch 800 | Accuracy 0.4726 | Loss 1.1756\n",
      "Epoch 7 | Batch 850 | Accuracy 0.4729 | Loss 1.1711\n",
      "Epoch 7 | Batch 900 | Accuracy 0.4731 | Loss 1.1665\n",
      "Epoch 7 | Batch 950 | Accuracy 0.4735 | Loss 1.1621\n",
      "Epoch 7 | Batch 1000 | Accuracy 0.4736 | Loss 1.1572\n",
      "Epoch 7 | Batch 1050 | Accuracy 0.4739 | Loss 1.1512\n",
      "Epoch 7 | Batch 1100 | Accuracy 0.4743 | Loss 1.1439\n",
      "Epoch 7 | Batch 1150 | Accuracy 0.4746 | Loss 1.1367\n",
      "Epoch 7 | Batch 1200 | Accuracy 0.4750 | Loss 1.1312\n",
      "Epoch 7 | Batch 1250 | Accuracy 0.4754 | Loss 1.1240\n",
      "Epoch 7 | Batch 1300 | Accuracy 0.4759 | Loss 1.1186\n",
      "Epoch 7 | Batch 1350 | Accuracy 0.4764 | Loss 1.1125\n",
      "Epoch 7 | Batch 1400 | Accuracy 0.4771 | Loss 1.1075\n",
      "Epoch 7 | Batch 1450 | Accuracy 0.4777 | Loss 1.1018\n",
      "Epoch 7 | Batch 1500 | Accuracy 0.4782 | Loss 1.0962\n",
      "Epoch 7 | Batch 1550 | Accuracy 0.4787 | Loss 1.0925\n",
      "Epoch 7 | Batch 1600 | Accuracy 0.4793 | Loss 1.0882\n",
      "Epoch 7 | Batch 1650 | Accuracy 0.4801 | Loss 1.0844\n",
      "Epoch 7 | Batch 1700 | Accuracy 0.4807 | Loss 1.0806\n",
      "Epoch 7 | Batch 1750 | Accuracy 0.4814 | Loss 1.0779\n",
      "Epoch 7 | Batch 1800 | Accuracy 0.4819 | Loss 1.0738\n",
      "Epoch 7 | Batch 1850 | Accuracy 0.4825 | Loss 1.0706\n",
      "Epoch 7 | Batch 1900 | Accuracy 0.4835 | Loss 1.0668\n",
      "Epoch 7 | Batch 1950 | Accuracy 0.4840 | Loss 1.0638\n",
      "Epoch 7 | Batch 2000 | Accuracy 0.4847 | Loss 1.0617\n",
      "Epoch 7 | Batch 2050 | Accuracy 0.4852 | Loss 1.0590\n",
      "Epoch 7 | Batch 2100 | Accuracy 0.4857 | Loss 1.0566\n",
      "Epoch 7 | Batch 2150 | Accuracy 0.4863 | Loss 1.0544\n",
      "Epoch 7 | Batch 2200 | Accuracy 0.4868 | Loss 1.0518\n",
      "Epoch 7 | Batch 2250 | Accuracy 0.4873 | Loss 1.0505\n",
      "Epoch 7 | Batch 2300 | Accuracy 0.4875 | Loss 1.0501\n",
      "Epoch 7 | Batch 2350 | Accuracy 0.4877 | Loss 1.0506\n",
      "Epoch 7 | Batch 2400 | Accuracy 0.4877 | Loss 1.0523\n",
      "Epoch 7 | Batch 2450 | Accuracy 0.4876 | Loss 1.0537\n",
      "Epoch 7 | Batch 2500 | Accuracy 0.4875 | Loss 1.0564\n",
      "Epoch 7 | Batch 2550 | Accuracy 0.4872 | Loss 1.0591\n",
      "Epoch 7 | Batch 2600 | Accuracy 0.4871 | Loss 1.0618\n",
      "Epoch 7 | Batch 2650 | Accuracy 0.4869 | Loss 1.0645\n",
      "Epoch 7 | Batch 2700 | Accuracy 0.4867 | Loss 1.0670\n",
      "Epoch 7 | Batch 2750 | Accuracy 0.4867 | Loss 1.0693\n",
      "Epoch 7 | Batch 2800 | Accuracy 0.4864 | Loss 1.0722\n",
      "Epoch 7 | Batch 2850 | Accuracy 0.4861 | Loss 1.0750\n",
      "Epoch 7 | Batch 2900 | Accuracy 0.4857 | Loss 1.0774\n",
      "Epoch 7 | Batch 2950 | Accuracy 0.4855 | Loss 1.0797\n",
      "Epoch 7 | Batch 3000 | Accuracy 0.4852 | Loss 1.0820\n",
      "Epoch 7 | Batch 3050 | Accuracy 0.4849 | Loss 1.0839\n",
      "Epoch 7 | Batch 3100 | Accuracy 0.4847 | Loss 1.0857\n",
      "Epoch 7 | Batch 3150 | Accuracy 0.4844 | Loss 1.0882\n",
      "Epoch 7 | Batch 3200 | Accuracy 0.4842 | Loss 1.0903\n",
      "Epoch 7 | Batch 3250 | Accuracy 0.4839 | Loss 1.0924\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 6: 331.9034345149994 secs \n",
      "\n",
      "Epoch 8 of 20\n",
      "Epoch 8 | Batch 0 | Accuracy 0.4911 | Loss 1.1595\n",
      "Epoch 8 | Batch 50 | Accuracy 0.4742 | Loss 1.2220\n",
      "Epoch 8 | Batch 100 | Accuracy 0.4730 | Loss 1.2196\n",
      "Epoch 8 | Batch 150 | Accuracy 0.4716 | Loss 1.2198\n",
      "Epoch 8 | Batch 200 | Accuracy 0.4709 | Loss 1.2129\n",
      "Epoch 8 | Batch 250 | Accuracy 0.4716 | Loss 1.2076\n",
      "Epoch 8 | Batch 300 | Accuracy 0.4721 | Loss 1.2036\n",
      "Epoch 8 | Batch 350 | Accuracy 0.4723 | Loss 1.1958\n",
      "Epoch 8 | Batch 400 | Accuracy 0.4735 | Loss 1.1914\n",
      "Epoch 8 | Batch 450 | Accuracy 0.4737 | Loss 1.1882\n",
      "Epoch 8 | Batch 500 | Accuracy 0.4744 | Loss 1.1808\n",
      "Epoch 8 | Batch 550 | Accuracy 0.4745 | Loss 1.1762\n",
      "Epoch 8 | Batch 600 | Accuracy 0.4743 | Loss 1.1713\n",
      "Epoch 8 | Batch 650 | Accuracy 0.4751 | Loss 1.1667\n",
      "Epoch 8 | Batch 700 | Accuracy 0.4753 | Loss 1.1601\n",
      "Epoch 8 | Batch 750 | Accuracy 0.4755 | Loss 1.1555\n",
      "Epoch 8 | Batch 800 | Accuracy 0.4757 | Loss 1.1510\n",
      "Epoch 8 | Batch 850 | Accuracy 0.4760 | Loss 1.1475\n",
      "Epoch 8 | Batch 900 | Accuracy 0.4765 | Loss 1.1432\n",
      "Epoch 8 | Batch 950 | Accuracy 0.4769 | Loss 1.1390\n",
      "Epoch 8 | Batch 1000 | Accuracy 0.4772 | Loss 1.1333\n",
      "Epoch 8 | Batch 1050 | Accuracy 0.4778 | Loss 1.1266\n",
      "Epoch 8 | Batch 1100 | Accuracy 0.4779 | Loss 1.1201\n",
      "Epoch 8 | Batch 1150 | Accuracy 0.4781 | Loss 1.1137\n",
      "Epoch 8 | Batch 1200 | Accuracy 0.4783 | Loss 1.1065\n",
      "Epoch 8 | Batch 1250 | Accuracy 0.4788 | Loss 1.1000\n",
      "Epoch 8 | Batch 1300 | Accuracy 0.4795 | Loss 1.0943\n",
      "Epoch 8 | Batch 1350 | Accuracy 0.4802 | Loss 1.0893\n",
      "Epoch 8 | Batch 1400 | Accuracy 0.4806 | Loss 1.0843\n",
      "Epoch 8 | Batch 1450 | Accuracy 0.4812 | Loss 1.0799\n",
      "Epoch 8 | Batch 1500 | Accuracy 0.4817 | Loss 1.0746\n",
      "Epoch 8 | Batch 1550 | Accuracy 0.4823 | Loss 1.0706\n",
      "Epoch 8 | Batch 1600 | Accuracy 0.4829 | Loss 1.0672\n",
      "Epoch 8 | Batch 1650 | Accuracy 0.4834 | Loss 1.0630\n",
      "Epoch 8 | Batch 1700 | Accuracy 0.4841 | Loss 1.0587\n",
      "Epoch 8 | Batch 1750 | Accuracy 0.4848 | Loss 1.0553\n",
      "Epoch 8 | Batch 1800 | Accuracy 0.4855 | Loss 1.0518\n",
      "Epoch 8 | Batch 1850 | Accuracy 0.4861 | Loss 1.0480\n",
      "Epoch 8 | Batch 1900 | Accuracy 0.4869 | Loss 1.0449\n",
      "Epoch 8 | Batch 1950 | Accuracy 0.4875 | Loss 1.0420\n",
      "Epoch 8 | Batch 2000 | Accuracy 0.4881 | Loss 1.0388\n",
      "Epoch 8 | Batch 2050 | Accuracy 0.4886 | Loss 1.0364\n",
      "Epoch 8 | Batch 2100 | Accuracy 0.4892 | Loss 1.0337\n",
      "Epoch 8 | Batch 2150 | Accuracy 0.4896 | Loss 1.0317\n",
      "Epoch 8 | Batch 2200 | Accuracy 0.4901 | Loss 1.0294\n",
      "Epoch 8 | Batch 2250 | Accuracy 0.4904 | Loss 1.0276\n",
      "Epoch 8 | Batch 2300 | Accuracy 0.4907 | Loss 1.0275\n",
      "Epoch 8 | Batch 2350 | Accuracy 0.4908 | Loss 1.0281\n",
      "Epoch 8 | Batch 2400 | Accuracy 0.4908 | Loss 1.0292\n",
      "Epoch 8 | Batch 2450 | Accuracy 0.4907 | Loss 1.0310\n",
      "Epoch 8 | Batch 2500 | Accuracy 0.4906 | Loss 1.0334\n",
      "Epoch 8 | Batch 2550 | Accuracy 0.4906 | Loss 1.0355\n",
      "Epoch 8 | Batch 2600 | Accuracy 0.4905 | Loss 1.0377\n",
      "Epoch 8 | Batch 2650 | Accuracy 0.4904 | Loss 1.0402\n",
      "Epoch 8 | Batch 2700 | Accuracy 0.4900 | Loss 1.0434\n",
      "Epoch 8 | Batch 2750 | Accuracy 0.4898 | Loss 1.0464\n",
      "Epoch 8 | Batch 2800 | Accuracy 0.4895 | Loss 1.0493\n",
      "Epoch 8 | Batch 2850 | Accuracy 0.4893 | Loss 1.0516\n",
      "Epoch 8 | Batch 2900 | Accuracy 0.4890 | Loss 1.0543\n",
      "Epoch 8 | Batch 2950 | Accuracy 0.4888 | Loss 1.0572\n",
      "Epoch 8 | Batch 3000 | Accuracy 0.4885 | Loss 1.0593\n",
      "Epoch 8 | Batch 3050 | Accuracy 0.4882 | Loss 1.0614\n",
      "Epoch 8 | Batch 3100 | Accuracy 0.4879 | Loss 1.0635\n",
      "Epoch 8 | Batch 3150 | Accuracy 0.4877 | Loss 1.0653\n",
      "Epoch 8 | Batch 3200 | Accuracy 0.4874 | Loss 1.0677\n",
      "Epoch 8 | Batch 3250 | Accuracy 0.4872 | Loss 1.0696\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 7: 330.59797620773315 secs \n",
      "\n",
      "Epoch 9 of 20\n",
      "Epoch 9 | Batch 0 | Accuracy 0.4877 | Loss 1.0562\n",
      "Epoch 9 | Batch 50 | Accuracy 0.4744 | Loss 1.2192\n",
      "Epoch 9 | Batch 100 | Accuracy 0.4724 | Loss 1.2097\n",
      "Epoch 9 | Batch 150 | Accuracy 0.4748 | Loss 1.2061\n",
      "Epoch 9 | Batch 200 | Accuracy 0.4761 | Loss 1.2003\n",
      "Epoch 9 | Batch 250 | Accuracy 0.4772 | Loss 1.1920\n",
      "Epoch 9 | Batch 300 | Accuracy 0.4775 | Loss 1.1908\n",
      "Epoch 9 | Batch 350 | Accuracy 0.4775 | Loss 1.1808\n",
      "Epoch 9 | Batch 400 | Accuracy 0.4781 | Loss 1.1673\n",
      "Epoch 9 | Batch 450 | Accuracy 0.4786 | Loss 1.1641\n",
      "Epoch 9 | Batch 500 | Accuracy 0.4787 | Loss 1.1589\n",
      "Epoch 9 | Batch 550 | Accuracy 0.4789 | Loss 1.1507\n",
      "Epoch 9 | Batch 600 | Accuracy 0.4791 | Loss 1.1454\n",
      "Epoch 9 | Batch 650 | Accuracy 0.4794 | Loss 1.1417\n",
      "Epoch 9 | Batch 700 | Accuracy 0.4795 | Loss 1.1369\n",
      "Epoch 9 | Batch 750 | Accuracy 0.4799 | Loss 1.1318\n",
      "Epoch 9 | Batch 800 | Accuracy 0.4800 | Loss 1.1292\n",
      "Epoch 9 | Batch 850 | Accuracy 0.4800 | Loss 1.1257\n",
      "Epoch 9 | Batch 900 | Accuracy 0.4802 | Loss 1.1219\n",
      "Epoch 9 | Batch 950 | Accuracy 0.4802 | Loss 1.1173\n",
      "Epoch 9 | Batch 1000 | Accuracy 0.4803 | Loss 1.1134\n",
      "Epoch 9 | Batch 1050 | Accuracy 0.4806 | Loss 1.1072\n",
      "Epoch 9 | Batch 1100 | Accuracy 0.4808 | Loss 1.1010\n",
      "Epoch 9 | Batch 1150 | Accuracy 0.4811 | Loss 1.0945\n",
      "Epoch 9 | Batch 1200 | Accuracy 0.4816 | Loss 1.0876\n",
      "Epoch 9 | Batch 1250 | Accuracy 0.4818 | Loss 1.0814\n",
      "Epoch 9 | Batch 1300 | Accuracy 0.4823 | Loss 1.0751\n",
      "Epoch 9 | Batch 1350 | Accuracy 0.4830 | Loss 1.0692\n",
      "Epoch 9 | Batch 1400 | Accuracy 0.4837 | Loss 1.0642\n",
      "Epoch 9 | Batch 1450 | Accuracy 0.4844 | Loss 1.0590\n",
      "Epoch 9 | Batch 1500 | Accuracy 0.4849 | Loss 1.0539\n",
      "Epoch 9 | Batch 1550 | Accuracy 0.4855 | Loss 1.0490\n",
      "Epoch 9 | Batch 1600 | Accuracy 0.4862 | Loss 1.0450\n",
      "Epoch 9 | Batch 1650 | Accuracy 0.4867 | Loss 1.0410\n",
      "Epoch 9 | Batch 1700 | Accuracy 0.4874 | Loss 1.0370\n",
      "Epoch 9 | Batch 1750 | Accuracy 0.4879 | Loss 1.0328\n",
      "Epoch 9 | Batch 1800 | Accuracy 0.4885 | Loss 1.0298\n",
      "Epoch 9 | Batch 1850 | Accuracy 0.4890 | Loss 1.0266\n",
      "Epoch 9 | Batch 1900 | Accuracy 0.4896 | Loss 1.0244\n",
      "Epoch 9 | Batch 1950 | Accuracy 0.4901 | Loss 1.0218\n",
      "Epoch 9 | Batch 2000 | Accuracy 0.4906 | Loss 1.0188\n",
      "Epoch 9 | Batch 2050 | Accuracy 0.4913 | Loss 1.0165\n",
      "Epoch 9 | Batch 2100 | Accuracy 0.4920 | Loss 1.0143\n",
      "Epoch 9 | Batch 2150 | Accuracy 0.4926 | Loss 1.0128\n",
      "Epoch 9 | Batch 2200 | Accuracy 0.4931 | Loss 1.0105\n",
      "Epoch 9 | Batch 2250 | Accuracy 0.4934 | Loss 1.0088\n",
      "Epoch 9 | Batch 2300 | Accuracy 0.4937 | Loss 1.0083\n",
      "Epoch 9 | Batch 2350 | Accuracy 0.4939 | Loss 1.0095\n",
      "Epoch 9 | Batch 2400 | Accuracy 0.4939 | Loss 1.0110\n",
      "Epoch 9 | Batch 2450 | Accuracy 0.4938 | Loss 1.0130\n",
      "Epoch 9 | Batch 2500 | Accuracy 0.4936 | Loss 1.0149\n",
      "Epoch 9 | Batch 2550 | Accuracy 0.4934 | Loss 1.0173\n",
      "Epoch 9 | Batch 2600 | Accuracy 0.4932 | Loss 1.0199\n",
      "Epoch 9 | Batch 2650 | Accuracy 0.4930 | Loss 1.0228\n",
      "Epoch 9 | Batch 2700 | Accuracy 0.4927 | Loss 1.0246\n",
      "Epoch 9 | Batch 2750 | Accuracy 0.4925 | Loss 1.0279\n",
      "Epoch 9 | Batch 2800 | Accuracy 0.4923 | Loss 1.0306\n",
      "Epoch 9 | Batch 2850 | Accuracy 0.4920 | Loss 1.0333\n",
      "Epoch 9 | Batch 2900 | Accuracy 0.4917 | Loss 1.0358\n",
      "Epoch 9 | Batch 2950 | Accuracy 0.4914 | Loss 1.0377\n",
      "Epoch 9 | Batch 3000 | Accuracy 0.4911 | Loss 1.0401\n",
      "Epoch 9 | Batch 3050 | Accuracy 0.4908 | Loss 1.0425\n",
      "Epoch 9 | Batch 3100 | Accuracy 0.4906 | Loss 1.0447\n",
      "Epoch 9 | Batch 3150 | Accuracy 0.4904 | Loss 1.0473\n",
      "Epoch 9 | Batch 3200 | Accuracy 0.4902 | Loss 1.0495\n",
      "Epoch 9 | Batch 3250 | Accuracy 0.4900 | Loss 1.0511\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 8: 332.9570596218109 secs \n",
      "\n",
      "Epoch 10 of 20\n",
      "Epoch 10 | Batch 0 | Accuracy 0.5011 | Loss 1.0799\n",
      "Epoch 10 | Batch 50 | Accuracy 0.4706 | Loss 1.2093\n",
      "Epoch 10 | Batch 100 | Accuracy 0.4733 | Loss 1.1971\n",
      "Epoch 10 | Batch 150 | Accuracy 0.4738 | Loss 1.1941\n",
      "Epoch 10 | Batch 200 | Accuracy 0.4761 | Loss 1.1813\n",
      "Epoch 10 | Batch 250 | Accuracy 0.4766 | Loss 1.1747\n",
      "Epoch 10 | Batch 300 | Accuracy 0.4771 | Loss 1.1715\n",
      "Epoch 10 | Batch 350 | Accuracy 0.4777 | Loss 1.1640\n",
      "Epoch 10 | Batch 400 | Accuracy 0.4782 | Loss 1.1587\n",
      "Epoch 10 | Batch 450 | Accuracy 0.4786 | Loss 1.1482\n",
      "Epoch 10 | Batch 500 | Accuracy 0.4791 | Loss 1.1420\n",
      "Epoch 10 | Batch 550 | Accuracy 0.4793 | Loss 1.1360\n",
      "Epoch 10 | Batch 600 | Accuracy 0.4799 | Loss 1.1322\n",
      "Epoch 10 | Batch 650 | Accuracy 0.4802 | Loss 1.1247\n",
      "Epoch 10 | Batch 700 | Accuracy 0.4808 | Loss 1.1188\n",
      "Epoch 10 | Batch 750 | Accuracy 0.4810 | Loss 1.1144\n",
      "Epoch 10 | Batch 800 | Accuracy 0.4812 | Loss 1.1103\n",
      "Epoch 10 | Batch 850 | Accuracy 0.4812 | Loss 1.1074\n",
      "Epoch 10 | Batch 900 | Accuracy 0.4817 | Loss 1.1024\n",
      "Epoch 10 | Batch 950 | Accuracy 0.4819 | Loss 1.0988\n",
      "Epoch 10 | Batch 1000 | Accuracy 0.4820 | Loss 1.0945\n",
      "Epoch 10 | Batch 1050 | Accuracy 0.4824 | Loss 1.0880\n",
      "Epoch 10 | Batch 1100 | Accuracy 0.4824 | Loss 1.0815\n",
      "Epoch 10 | Batch 1150 | Accuracy 0.4829 | Loss 1.0761\n",
      "Epoch 10 | Batch 1200 | Accuracy 0.4835 | Loss 1.0701\n",
      "Epoch 10 | Batch 1250 | Accuracy 0.4838 | Loss 1.0646\n",
      "Epoch 10 | Batch 1300 | Accuracy 0.4842 | Loss 1.0581\n",
      "Epoch 10 | Batch 1350 | Accuracy 0.4849 | Loss 1.0529\n",
      "Epoch 10 | Batch 1400 | Accuracy 0.4854 | Loss 1.0479\n",
      "Epoch 10 | Batch 1450 | Accuracy 0.4861 | Loss 1.0425\n",
      "Epoch 10 | Batch 1500 | Accuracy 0.4867 | Loss 1.0378\n",
      "Epoch 10 | Batch 1550 | Accuracy 0.4872 | Loss 1.0342\n",
      "Epoch 10 | Batch 1600 | Accuracy 0.4879 | Loss 1.0301\n",
      "Epoch 10 | Batch 1650 | Accuracy 0.4884 | Loss 1.0262\n",
      "Epoch 10 | Batch 1700 | Accuracy 0.4890 | Loss 1.0225\n",
      "Epoch 10 | Batch 1750 | Accuracy 0.4895 | Loss 1.0186\n",
      "Epoch 10 | Batch 1800 | Accuracy 0.4901 | Loss 1.0152\n",
      "Epoch 10 | Batch 1850 | Accuracy 0.4907 | Loss 1.0123\n",
      "Epoch 10 | Batch 1900 | Accuracy 0.4913 | Loss 1.0101\n",
      "Epoch 10 | Batch 1950 | Accuracy 0.4919 | Loss 1.0074\n",
      "Epoch 10 | Batch 2000 | Accuracy 0.4926 | Loss 1.0043\n",
      "Epoch 10 | Batch 2050 | Accuracy 0.4932 | Loss 1.0015\n",
      "Epoch 10 | Batch 2100 | Accuracy 0.4938 | Loss 0.9995\n",
      "Epoch 10 | Batch 2150 | Accuracy 0.4945 | Loss 0.9972\n",
      "Epoch 10 | Batch 2200 | Accuracy 0.4950 | Loss 0.9950\n",
      "Epoch 10 | Batch 2250 | Accuracy 0.4954 | Loss 0.9939\n",
      "Epoch 10 | Batch 2300 | Accuracy 0.4957 | Loss 0.9938\n",
      "Epoch 10 | Batch 2350 | Accuracy 0.4957 | Loss 0.9942\n",
      "Epoch 10 | Batch 2400 | Accuracy 0.4957 | Loss 0.9955\n",
      "Epoch 10 | Batch 2450 | Accuracy 0.4957 | Loss 0.9971\n",
      "Epoch 10 | Batch 2500 | Accuracy 0.4956 | Loss 0.9992\n",
      "Epoch 10 | Batch 2550 | Accuracy 0.4954 | Loss 1.0018\n",
      "Epoch 10 | Batch 2600 | Accuracy 0.4952 | Loss 1.0041\n",
      "Epoch 10 | Batch 2650 | Accuracy 0.4950 | Loss 1.0065\n",
      "Epoch 10 | Batch 2700 | Accuracy 0.4947 | Loss 1.0092\n",
      "Epoch 10 | Batch 2750 | Accuracy 0.4947 | Loss 1.0117\n",
      "Epoch 10 | Batch 2800 | Accuracy 0.4945 | Loss 1.0148\n",
      "Epoch 10 | Batch 2850 | Accuracy 0.4942 | Loss 1.0174\n",
      "Epoch 10 | Batch 2900 | Accuracy 0.4939 | Loss 1.0204\n",
      "Epoch 10 | Batch 2950 | Accuracy 0.4936 | Loss 1.0229\n",
      "Epoch 10 | Batch 3000 | Accuracy 0.4933 | Loss 1.0259\n",
      "Epoch 10 | Batch 3050 | Accuracy 0.4930 | Loss 1.0280\n",
      "Epoch 10 | Batch 3100 | Accuracy 0.4927 | Loss 1.0300\n",
      "Epoch 10 | Batch 3150 | Accuracy 0.4925 | Loss 1.0321\n",
      "Epoch 10 | Batch 3200 | Accuracy 0.4923 | Loss 1.0337\n",
      "Epoch 10 | Batch 3250 | Accuracy 0.4921 | Loss 1.0358\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 9: 329.18693256378174 secs \n",
      "\n",
      "Epoch 11 of 20\n",
      "Epoch 11 | Batch 0 | Accuracy 0.4643 | Loss 1.1755\n",
      "Epoch 11 | Batch 50 | Accuracy 0.4746 | Loss 1.2091\n",
      "Epoch 11 | Batch 100 | Accuracy 0.4793 | Loss 1.1944\n",
      "Epoch 11 | Batch 150 | Accuracy 0.4793 | Loss 1.1795\n",
      "Epoch 11 | Batch 200 | Accuracy 0.4788 | Loss 1.1703\n",
      "Epoch 11 | Batch 250 | Accuracy 0.4812 | Loss 1.1637\n",
      "Epoch 11 | Batch 300 | Accuracy 0.4802 | Loss 1.1569\n",
      "Epoch 11 | Batch 350 | Accuracy 0.4810 | Loss 1.1504\n",
      "Epoch 11 | Batch 400 | Accuracy 0.4812 | Loss 1.1410\n",
      "Epoch 11 | Batch 450 | Accuracy 0.4814 | Loss 1.1347\n",
      "Epoch 11 | Batch 500 | Accuracy 0.4817 | Loss 1.1257\n",
      "Epoch 11 | Batch 550 | Accuracy 0.4818 | Loss 1.1200\n",
      "Epoch 11 | Batch 600 | Accuracy 0.4819 | Loss 1.1161\n",
      "Epoch 11 | Batch 650 | Accuracy 0.4826 | Loss 1.1116\n",
      "Epoch 11 | Batch 700 | Accuracy 0.4831 | Loss 1.1078\n",
      "Epoch 11 | Batch 750 | Accuracy 0.4834 | Loss 1.1032\n",
      "Epoch 11 | Batch 800 | Accuracy 0.4834 | Loss 1.1005\n",
      "Epoch 11 | Batch 850 | Accuracy 0.4835 | Loss 1.0968\n",
      "Epoch 11 | Batch 900 | Accuracy 0.4838 | Loss 1.0925\n",
      "Epoch 11 | Batch 950 | Accuracy 0.4841 | Loss 1.0875\n",
      "Epoch 11 | Batch 1000 | Accuracy 0.4847 | Loss 1.0832\n",
      "Epoch 11 | Batch 1050 | Accuracy 0.4846 | Loss 1.0762\n",
      "Epoch 11 | Batch 1100 | Accuracy 0.4851 | Loss 1.0691\n",
      "Epoch 11 | Batch 1150 | Accuracy 0.4851 | Loss 1.0625\n",
      "Epoch 11 | Batch 1200 | Accuracy 0.4857 | Loss 1.0555\n",
      "Epoch 11 | Batch 1250 | Accuracy 0.4861 | Loss 1.0488\n",
      "Epoch 11 | Batch 1300 | Accuracy 0.4866 | Loss 1.0433\n",
      "Epoch 11 | Batch 1350 | Accuracy 0.4873 | Loss 1.0381\n",
      "Epoch 11 | Batch 1400 | Accuracy 0.4880 | Loss 1.0336\n",
      "Epoch 11 | Batch 1450 | Accuracy 0.4888 | Loss 1.0286\n",
      "Epoch 11 | Batch 1500 | Accuracy 0.4894 | Loss 1.0240\n",
      "Epoch 11 | Batch 1550 | Accuracy 0.4899 | Loss 1.0206\n",
      "Epoch 11 | Batch 1600 | Accuracy 0.4905 | Loss 1.0159\n",
      "Epoch 11 | Batch 1650 | Accuracy 0.4910 | Loss 1.0129\n",
      "Epoch 11 | Batch 1700 | Accuracy 0.4917 | Loss 1.0092\n",
      "Epoch 11 | Batch 1750 | Accuracy 0.4922 | Loss 1.0059\n",
      "Epoch 11 | Batch 1800 | Accuracy 0.4928 | Loss 1.0025\n",
      "Epoch 11 | Batch 1850 | Accuracy 0.4933 | Loss 0.9994\n",
      "Epoch 11 | Batch 1900 | Accuracy 0.4938 | Loss 0.9972\n",
      "Epoch 11 | Batch 1950 | Accuracy 0.4945 | Loss 0.9939\n",
      "Epoch 11 | Batch 2000 | Accuracy 0.4951 | Loss 0.9911\n",
      "Epoch 11 | Batch 2050 | Accuracy 0.4957 | Loss 0.9887\n",
      "Epoch 11 | Batch 2100 | Accuracy 0.4962 | Loss 0.9864\n",
      "Epoch 11 | Batch 2150 | Accuracy 0.4967 | Loss 0.9838\n",
      "Epoch 11 | Batch 2200 | Accuracy 0.4974 | Loss 0.9819\n",
      "Epoch 11 | Batch 2250 | Accuracy 0.4977 | Loss 0.9802\n",
      "Epoch 11 | Batch 2300 | Accuracy 0.4979 | Loss 0.9798\n",
      "Epoch 11 | Batch 2350 | Accuracy 0.4979 | Loss 0.9803\n",
      "Epoch 11 | Batch 2400 | Accuracy 0.4979 | Loss 0.9813\n",
      "Epoch 11 | Batch 2450 | Accuracy 0.4979 | Loss 0.9825\n",
      "Epoch 11 | Batch 2500 | Accuracy 0.4978 | Loss 0.9850\n",
      "Epoch 11 | Batch 2550 | Accuracy 0.4976 | Loss 0.9869\n",
      "Epoch 11 | Batch 2600 | Accuracy 0.4975 | Loss 0.9893\n",
      "Epoch 11 | Batch 2650 | Accuracy 0.4975 | Loss 0.9916\n",
      "Epoch 11 | Batch 2700 | Accuracy 0.4973 | Loss 0.9947\n",
      "Epoch 11 | Batch 2750 | Accuracy 0.4971 | Loss 0.9976\n",
      "Epoch 11 | Batch 2800 | Accuracy 0.4968 | Loss 1.0004\n",
      "Epoch 11 | Batch 2850 | Accuracy 0.4965 | Loss 1.0028\n",
      "Epoch 11 | Batch 2900 | Accuracy 0.4963 | Loss 1.0057\n",
      "Epoch 11 | Batch 2950 | Accuracy 0.4960 | Loss 1.0084\n",
      "Epoch 11 | Batch 3000 | Accuracy 0.4958 | Loss 1.0110\n",
      "Epoch 11 | Batch 3050 | Accuracy 0.4954 | Loss 1.0136\n",
      "Epoch 11 | Batch 3100 | Accuracy 0.4951 | Loss 1.0163\n",
      "Epoch 11 | Batch 3150 | Accuracy 0.4949 | Loss 1.0182\n",
      "Epoch 11 | Batch 3200 | Accuracy 0.4947 | Loss 1.0204\n",
      "Epoch 11 | Batch 3250 | Accuracy 0.4945 | Loss 1.0221\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 10: 324.49587178230286 secs \n",
      "\n",
      "Epoch 12 of 20\n",
      "Epoch 12 | Batch 0 | Accuracy 0.5011 | Loss 1.1612\n",
      "Epoch 12 | Batch 50 | Accuracy 0.4839 | Loss 1.1601\n",
      "Epoch 12 | Batch 100 | Accuracy 0.4793 | Loss 1.1555\n",
      "Epoch 12 | Batch 150 | Accuracy 0.4789 | Loss 1.1515\n",
      "Epoch 12 | Batch 200 | Accuracy 0.4805 | Loss 1.1495\n",
      "Epoch 12 | Batch 250 | Accuracy 0.4813 | Loss 1.1460\n",
      "Epoch 12 | Batch 300 | Accuracy 0.4813 | Loss 1.1417\n",
      "Epoch 12 | Batch 350 | Accuracy 0.4822 | Loss 1.1372\n",
      "Epoch 12 | Batch 400 | Accuracy 0.4822 | Loss 1.1328\n",
      "Epoch 12 | Batch 450 | Accuracy 0.4826 | Loss 1.1247\n",
      "Epoch 12 | Batch 500 | Accuracy 0.4837 | Loss 1.1181\n",
      "Epoch 12 | Batch 550 | Accuracy 0.4840 | Loss 1.1130\n",
      "Epoch 12 | Batch 600 | Accuracy 0.4842 | Loss 1.1083\n",
      "Epoch 12 | Batch 650 | Accuracy 0.4847 | Loss 1.1029\n",
      "Epoch 12 | Batch 700 | Accuracy 0.4847 | Loss 1.0964\n",
      "Epoch 12 | Batch 750 | Accuracy 0.4850 | Loss 1.0903\n",
      "Epoch 12 | Batch 800 | Accuracy 0.4856 | Loss 1.0865\n",
      "Epoch 12 | Batch 850 | Accuracy 0.4856 | Loss 1.0821\n",
      "Epoch 12 | Batch 900 | Accuracy 0.4859 | Loss 1.0791\n",
      "Epoch 12 | Batch 950 | Accuracy 0.4860 | Loss 1.0754\n",
      "Epoch 12 | Batch 1000 | Accuracy 0.4863 | Loss 1.0700\n",
      "Epoch 12 | Batch 1050 | Accuracy 0.4868 | Loss 1.0638\n",
      "Epoch 12 | Batch 1100 | Accuracy 0.4871 | Loss 1.0575\n",
      "Epoch 12 | Batch 1150 | Accuracy 0.4874 | Loss 1.0500\n",
      "Epoch 12 | Batch 1200 | Accuracy 0.4879 | Loss 1.0435\n",
      "Epoch 12 | Batch 1250 | Accuracy 0.4881 | Loss 1.0372\n",
      "Epoch 12 | Batch 1300 | Accuracy 0.4885 | Loss 1.0315\n",
      "Epoch 12 | Batch 1350 | Accuracy 0.4890 | Loss 1.0265\n",
      "Epoch 12 | Batch 1400 | Accuracy 0.4896 | Loss 1.0209\n",
      "Epoch 12 | Batch 1450 | Accuracy 0.4901 | Loss 1.0159\n",
      "Epoch 12 | Batch 1500 | Accuracy 0.4907 | Loss 1.0120\n",
      "Epoch 12 | Batch 1550 | Accuracy 0.4914 | Loss 1.0079\n",
      "Epoch 12 | Batch 1600 | Accuracy 0.4921 | Loss 1.0047\n",
      "Epoch 12 | Batch 1650 | Accuracy 0.4926 | Loss 1.0008\n",
      "Epoch 12 | Batch 1700 | Accuracy 0.4933 | Loss 0.9961\n",
      "Epoch 12 | Batch 1750 | Accuracy 0.4937 | Loss 0.9926\n",
      "Epoch 12 | Batch 1800 | Accuracy 0.4943 | Loss 0.9896\n",
      "Epoch 12 | Batch 1850 | Accuracy 0.4948 | Loss 0.9869\n",
      "Epoch 12 | Batch 1900 | Accuracy 0.4952 | Loss 0.9841\n",
      "Epoch 12 | Batch 1950 | Accuracy 0.4959 | Loss 0.9816\n",
      "Epoch 12 | Batch 2000 | Accuracy 0.4967 | Loss 0.9788\n",
      "Epoch 12 | Batch 2050 | Accuracy 0.4972 | Loss 0.9765\n",
      "Epoch 12 | Batch 2100 | Accuracy 0.4979 | Loss 0.9738\n",
      "Epoch 12 | Batch 2150 | Accuracy 0.4986 | Loss 0.9721\n",
      "Epoch 12 | Batch 2200 | Accuracy 0.4991 | Loss 0.9699\n",
      "Epoch 12 | Batch 2250 | Accuracy 0.4996 | Loss 0.9683\n",
      "Epoch 12 | Batch 2300 | Accuracy 0.4999 | Loss 0.9679\n",
      "Epoch 12 | Batch 2350 | Accuracy 0.5000 | Loss 0.9677\n",
      "Epoch 12 | Batch 2400 | Accuracy 0.5000 | Loss 0.9695\n",
      "Epoch 12 | Batch 2450 | Accuracy 0.4998 | Loss 0.9713\n",
      "Epoch 12 | Batch 2500 | Accuracy 0.4998 | Loss 0.9736\n",
      "Epoch 12 | Batch 2550 | Accuracy 0.4996 | Loss 0.9766\n",
      "Epoch 12 | Batch 2600 | Accuracy 0.4994 | Loss 0.9788\n",
      "Epoch 12 | Batch 2650 | Accuracy 0.4991 | Loss 0.9815\n",
      "Epoch 12 | Batch 2700 | Accuracy 0.4989 | Loss 0.9842\n",
      "Epoch 12 | Batch 2750 | Accuracy 0.4988 | Loss 0.9873\n",
      "Epoch 12 | Batch 2800 | Accuracy 0.4985 | Loss 0.9904\n",
      "Epoch 12 | Batch 2850 | Accuracy 0.4983 | Loss 0.9929\n",
      "Epoch 12 | Batch 2900 | Accuracy 0.4981 | Loss 0.9951\n",
      "Epoch 12 | Batch 2950 | Accuracy 0.4978 | Loss 0.9973\n",
      "Epoch 12 | Batch 3000 | Accuracy 0.4974 | Loss 0.9999\n",
      "Epoch 12 | Batch 3050 | Accuracy 0.4972 | Loss 1.0023\n",
      "Epoch 12 | Batch 3100 | Accuracy 0.4968 | Loss 1.0045\n",
      "Epoch 12 | Batch 3150 | Accuracy 0.4966 | Loss 1.0069\n",
      "Epoch 12 | Batch 3200 | Accuracy 0.4963 | Loss 1.0092\n",
      "Epoch 12 | Batch 3250 | Accuracy 0.4961 | Loss 1.0112\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 11: 334.64472556114197 secs \n",
      "\n",
      "Epoch 13 of 20\n",
      "Epoch 13 | Batch 0 | Accuracy 0.4855 | Loss 1.5218\n",
      "Epoch 13 | Batch 50 | Accuracy 0.4864 | Loss 1.1936\n",
      "Epoch 13 | Batch 100 | Accuracy 0.4837 | Loss 1.1728\n",
      "Epoch 13 | Batch 150 | Accuracy 0.4836 | Loss 1.1572\n",
      "Epoch 13 | Batch 200 | Accuracy 0.4846 | Loss 1.1481\n",
      "Epoch 13 | Batch 250 | Accuracy 0.4855 | Loss 1.1466\n",
      "Epoch 13 | Batch 300 | Accuracy 0.4862 | Loss 1.1362\n",
      "Epoch 13 | Batch 350 | Accuracy 0.4858 | Loss 1.1288\n",
      "Epoch 13 | Batch 400 | Accuracy 0.4861 | Loss 1.1230\n",
      "Epoch 13 | Batch 450 | Accuracy 0.4862 | Loss 1.1160\n",
      "Epoch 13 | Batch 500 | Accuracy 0.4864 | Loss 1.1090\n",
      "Epoch 13 | Batch 550 | Accuracy 0.4864 | Loss 1.1034\n",
      "Epoch 13 | Batch 600 | Accuracy 0.4863 | Loss 1.0980\n",
      "Epoch 13 | Batch 650 | Accuracy 0.4863 | Loss 1.0931\n",
      "Epoch 13 | Batch 700 | Accuracy 0.4864 | Loss 1.0880\n",
      "Epoch 13 | Batch 750 | Accuracy 0.4865 | Loss 1.0822\n",
      "Epoch 13 | Batch 800 | Accuracy 0.4870 | Loss 1.0788\n",
      "Epoch 13 | Batch 850 | Accuracy 0.4877 | Loss 1.0736\n",
      "Epoch 13 | Batch 900 | Accuracy 0.4878 | Loss 1.0704\n",
      "Epoch 13 | Batch 950 | Accuracy 0.4878 | Loss 1.0655\n",
      "Epoch 13 | Batch 1000 | Accuracy 0.4879 | Loss 1.0597\n",
      "Epoch 13 | Batch 1050 | Accuracy 0.4880 | Loss 1.0534\n",
      "Epoch 13 | Batch 1100 | Accuracy 0.4882 | Loss 1.0471\n",
      "Epoch 13 | Batch 1150 | Accuracy 0.4884 | Loss 1.0406\n",
      "Epoch 13 | Batch 1200 | Accuracy 0.4887 | Loss 1.0342\n",
      "Epoch 13 | Batch 1250 | Accuracy 0.4892 | Loss 1.0285\n",
      "Epoch 13 | Batch 1300 | Accuracy 0.4896 | Loss 1.0224\n",
      "Epoch 13 | Batch 1350 | Accuracy 0.4900 | Loss 1.0180\n",
      "Epoch 13 | Batch 1400 | Accuracy 0.4906 | Loss 1.0128\n",
      "Epoch 13 | Batch 1450 | Accuracy 0.4913 | Loss 1.0083\n",
      "Epoch 13 | Batch 1500 | Accuracy 0.4917 | Loss 1.0038\n",
      "Epoch 13 | Batch 1550 | Accuracy 0.4922 | Loss 0.9992\n",
      "Epoch 13 | Batch 1600 | Accuracy 0.4927 | Loss 0.9965\n",
      "Epoch 13 | Batch 1650 | Accuracy 0.4933 | Loss 0.9924\n",
      "Epoch 13 | Batch 1700 | Accuracy 0.4940 | Loss 0.9883\n",
      "Epoch 13 | Batch 1750 | Accuracy 0.4947 | Loss 0.9844\n",
      "Epoch 13 | Batch 1800 | Accuracy 0.4953 | Loss 0.9819\n",
      "Epoch 13 | Batch 1850 | Accuracy 0.4960 | Loss 0.9794\n",
      "Epoch 13 | Batch 1900 | Accuracy 0.4966 | Loss 0.9763\n",
      "Epoch 13 | Batch 1950 | Accuracy 0.4972 | Loss 0.9732\n",
      "Epoch 13 | Batch 2000 | Accuracy 0.4978 | Loss 0.9705\n",
      "Epoch 13 | Batch 2050 | Accuracy 0.4984 | Loss 0.9685\n",
      "Epoch 13 | Batch 2100 | Accuracy 0.4990 | Loss 0.9662\n",
      "Epoch 13 | Batch 2150 | Accuracy 0.4994 | Loss 0.9640\n",
      "Epoch 13 | Batch 2200 | Accuracy 0.5000 | Loss 0.9619\n",
      "Epoch 13 | Batch 2250 | Accuracy 0.5004 | Loss 0.9605\n",
      "Epoch 13 | Batch 2300 | Accuracy 0.5006 | Loss 0.9601\n",
      "Epoch 13 | Batch 2350 | Accuracy 0.5006 | Loss 0.9604\n",
      "Epoch 13 | Batch 2400 | Accuracy 0.5006 | Loss 0.9621\n",
      "Epoch 13 | Batch 2450 | Accuracy 0.5006 | Loss 0.9637\n",
      "Epoch 13 | Batch 2500 | Accuracy 0.5005 | Loss 0.9656\n",
      "Epoch 13 | Batch 2550 | Accuracy 0.5003 | Loss 0.9687\n",
      "Epoch 13 | Batch 2600 | Accuracy 0.5002 | Loss 0.9713\n",
      "Epoch 13 | Batch 2650 | Accuracy 0.5000 | Loss 0.9737\n",
      "Epoch 13 | Batch 2700 | Accuracy 0.5000 | Loss 0.9760\n",
      "Epoch 13 | Batch 2750 | Accuracy 0.4999 | Loss 0.9793\n",
      "Epoch 13 | Batch 2800 | Accuracy 0.4997 | Loss 0.9822\n",
      "Epoch 13 | Batch 2850 | Accuracy 0.4994 | Loss 0.9849\n",
      "Epoch 13 | Batch 2900 | Accuracy 0.4991 | Loss 0.9872\n",
      "Epoch 13 | Batch 2950 | Accuracy 0.4988 | Loss 0.9897\n",
      "Epoch 13 | Batch 3000 | Accuracy 0.4984 | Loss 0.9920\n",
      "Epoch 13 | Batch 3050 | Accuracy 0.4981 | Loss 0.9943\n",
      "Epoch 13 | Batch 3100 | Accuracy 0.4978 | Loss 0.9964\n",
      "Epoch 13 | Batch 3150 | Accuracy 0.4976 | Loss 0.9980\n",
      "Epoch 13 | Batch 3200 | Accuracy 0.4975 | Loss 0.9998\n",
      "Epoch 13 | Batch 3250 | Accuracy 0.4973 | Loss 1.0022\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 12: 336.624116897583 secs \n",
      "\n",
      "Epoch 14 of 20\n",
      "Epoch 14 | Batch 0 | Accuracy 0.4621 | Loss 1.2351\n",
      "Epoch 14 | Batch 50 | Accuracy 0.4813 | Loss 1.1343\n",
      "Epoch 14 | Batch 100 | Accuracy 0.4821 | Loss 1.1354\n",
      "Epoch 14 | Batch 150 | Accuracy 0.4832 | Loss 1.1323\n",
      "Epoch 14 | Batch 200 | Accuracy 0.4842 | Loss 1.1337\n",
      "Epoch 14 | Batch 250 | Accuracy 0.4852 | Loss 1.1297\n",
      "Epoch 14 | Batch 300 | Accuracy 0.4857 | Loss 1.1192\n",
      "Epoch 14 | Batch 350 | Accuracy 0.4859 | Loss 1.1151\n",
      "Epoch 14 | Batch 400 | Accuracy 0.4866 | Loss 1.1090\n",
      "Epoch 14 | Batch 450 | Accuracy 0.4868 | Loss 1.1020\n",
      "Epoch 14 | Batch 500 | Accuracy 0.4873 | Loss 1.0971\n",
      "Epoch 14 | Batch 550 | Accuracy 0.4873 | Loss 1.0902\n",
      "Epoch 14 | Batch 600 | Accuracy 0.4876 | Loss 1.0828\n",
      "Epoch 14 | Batch 650 | Accuracy 0.4875 | Loss 1.0786\n",
      "Epoch 14 | Batch 700 | Accuracy 0.4880 | Loss 1.0731\n",
      "Epoch 14 | Batch 750 | Accuracy 0.4887 | Loss 1.0682\n",
      "Epoch 14 | Batch 800 | Accuracy 0.4890 | Loss 1.0662\n",
      "Epoch 14 | Batch 850 | Accuracy 0.4887 | Loss 1.0625\n",
      "Epoch 14 | Batch 900 | Accuracy 0.4887 | Loss 1.0576\n",
      "Epoch 14 | Batch 950 | Accuracy 0.4890 | Loss 1.0519\n",
      "Epoch 14 | Batch 1000 | Accuracy 0.4891 | Loss 1.0471\n",
      "Epoch 14 | Batch 1050 | Accuracy 0.4895 | Loss 1.0430\n",
      "Epoch 14 | Batch 1100 | Accuracy 0.4897 | Loss 1.0356\n",
      "Epoch 14 | Batch 1150 | Accuracy 0.4902 | Loss 1.0291\n",
      "Epoch 14 | Batch 1200 | Accuracy 0.4906 | Loss 1.0239\n",
      "Epoch 14 | Batch 1250 | Accuracy 0.4909 | Loss 1.0172\n",
      "Epoch 14 | Batch 1300 | Accuracy 0.4915 | Loss 1.0120\n",
      "Epoch 14 | Batch 1350 | Accuracy 0.4922 | Loss 1.0065\n",
      "Epoch 14 | Batch 1400 | Accuracy 0.4926 | Loss 1.0012\n",
      "Epoch 14 | Batch 1450 | Accuracy 0.4932 | Loss 0.9969\n",
      "Epoch 14 | Batch 1500 | Accuracy 0.4939 | Loss 0.9929\n",
      "Epoch 14 | Batch 1550 | Accuracy 0.4946 | Loss 0.9882\n",
      "Epoch 14 | Batch 1600 | Accuracy 0.4951 | Loss 0.9844\n",
      "Epoch 14 | Batch 1650 | Accuracy 0.4957 | Loss 0.9802\n",
      "Epoch 14 | Batch 1700 | Accuracy 0.4963 | Loss 0.9763\n",
      "Epoch 14 | Batch 1750 | Accuracy 0.4967 | Loss 0.9734\n",
      "Epoch 14 | Batch 1800 | Accuracy 0.4972 | Loss 0.9706\n",
      "Epoch 14 | Batch 1850 | Accuracy 0.4979 | Loss 0.9678\n",
      "Epoch 14 | Batch 1900 | Accuracy 0.4985 | Loss 0.9655\n",
      "Epoch 14 | Batch 1950 | Accuracy 0.4993 | Loss 0.9632\n",
      "Epoch 14 | Batch 2000 | Accuracy 0.4998 | Loss 0.9605\n",
      "Epoch 14 | Batch 2050 | Accuracy 0.5003 | Loss 0.9578\n",
      "Epoch 14 | Batch 2100 | Accuracy 0.5009 | Loss 0.9556\n",
      "Epoch 14 | Batch 2150 | Accuracy 0.5015 | Loss 0.9537\n",
      "Epoch 14 | Batch 2200 | Accuracy 0.5020 | Loss 0.9519\n",
      "Epoch 14 | Batch 2250 | Accuracy 0.5023 | Loss 0.9501\n",
      "Epoch 14 | Batch 2300 | Accuracy 0.5026 | Loss 0.9500\n",
      "Epoch 14 | Batch 2350 | Accuracy 0.5027 | Loss 0.9501\n",
      "Epoch 14 | Batch 2400 | Accuracy 0.5028 | Loss 0.9515\n",
      "Epoch 14 | Batch 2450 | Accuracy 0.5027 | Loss 0.9533\n",
      "Epoch 14 | Batch 2500 | Accuracy 0.5026 | Loss 0.9556\n",
      "Epoch 14 | Batch 2550 | Accuracy 0.5023 | Loss 0.9581\n",
      "Epoch 14 | Batch 2600 | Accuracy 0.5021 | Loss 0.9599\n",
      "Epoch 14 | Batch 2650 | Accuracy 0.5019 | Loss 0.9623\n",
      "Epoch 14 | Batch 2700 | Accuracy 0.5017 | Loss 0.9655\n",
      "Epoch 14 | Batch 2750 | Accuracy 0.5016 | Loss 0.9683\n",
      "Epoch 14 | Batch 2800 | Accuracy 0.5013 | Loss 0.9710\n",
      "Epoch 14 | Batch 2850 | Accuracy 0.5011 | Loss 0.9734\n",
      "Epoch 14 | Batch 2900 | Accuracy 0.5006 | Loss 0.9765\n",
      "Epoch 14 | Batch 2950 | Accuracy 0.5004 | Loss 0.9786\n",
      "Epoch 14 | Batch 3000 | Accuracy 0.5001 | Loss 0.9811\n",
      "Epoch 14 | Batch 3050 | Accuracy 0.4997 | Loss 0.9837\n",
      "Epoch 14 | Batch 3100 | Accuracy 0.4994 | Loss 0.9860\n",
      "Epoch 14 | Batch 3150 | Accuracy 0.4992 | Loss 0.9886\n",
      "Epoch 14 | Batch 3200 | Accuracy 0.4989 | Loss 0.9909\n",
      "Epoch 14 | Batch 3250 | Accuracy 0.4987 | Loss 0.9932\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 13: 336.1022868156433 secs \n",
      "\n",
      "Epoch 15 of 20\n",
      "Epoch 15 | Batch 0 | Accuracy 0.4721 | Loss 1.3096\n",
      "Epoch 15 | Batch 50 | Accuracy 0.4844 | Loss 1.1414\n",
      "Epoch 15 | Batch 100 | Accuracy 0.4834 | Loss 1.1326\n",
      "Epoch 15 | Batch 150 | Accuracy 0.4831 | Loss 1.1348\n",
      "Epoch 15 | Batch 200 | Accuracy 0.4836 | Loss 1.1312\n",
      "Epoch 15 | Batch 250 | Accuracy 0.4847 | Loss 1.1261\n",
      "Epoch 15 | Batch 300 | Accuracy 0.4858 | Loss 1.1179\n",
      "Epoch 15 | Batch 350 | Accuracy 0.4868 | Loss 1.1111\n",
      "Epoch 15 | Batch 400 | Accuracy 0.4870 | Loss 1.1053\n",
      "Epoch 15 | Batch 450 | Accuracy 0.4872 | Loss 1.0984\n",
      "Epoch 15 | Batch 500 | Accuracy 0.4884 | Loss 1.0909\n",
      "Epoch 15 | Batch 550 | Accuracy 0.4884 | Loss 1.0831\n",
      "Epoch 15 | Batch 600 | Accuracy 0.4888 | Loss 1.0788\n",
      "Epoch 15 | Batch 650 | Accuracy 0.4896 | Loss 1.0740\n",
      "Epoch 15 | Batch 700 | Accuracy 0.4900 | Loss 1.0685\n",
      "Epoch 15 | Batch 750 | Accuracy 0.4901 | Loss 1.0633\n",
      "Epoch 15 | Batch 800 | Accuracy 0.4901 | Loss 1.0579\n",
      "Epoch 15 | Batch 850 | Accuracy 0.4903 | Loss 1.0532\n",
      "Epoch 15 | Batch 900 | Accuracy 0.4907 | Loss 1.0493\n",
      "Epoch 15 | Batch 950 | Accuracy 0.4913 | Loss 1.0459\n",
      "Epoch 15 | Batch 1000 | Accuracy 0.4912 | Loss 1.0397\n",
      "Epoch 15 | Batch 1050 | Accuracy 0.4912 | Loss 1.0337\n",
      "Epoch 15 | Batch 1100 | Accuracy 0.4913 | Loss 1.0277\n",
      "Epoch 15 | Batch 1150 | Accuracy 0.4915 | Loss 1.0206\n",
      "Epoch 15 | Batch 1200 | Accuracy 0.4921 | Loss 1.0154\n",
      "Epoch 15 | Batch 1250 | Accuracy 0.4924 | Loss 1.0096\n",
      "Epoch 15 | Batch 1300 | Accuracy 0.4928 | Loss 1.0037\n",
      "Epoch 15 | Batch 1350 | Accuracy 0.4933 | Loss 0.9983\n",
      "Epoch 15 | Batch 1400 | Accuracy 0.4939 | Loss 0.9935\n",
      "Epoch 15 | Batch 1450 | Accuracy 0.4944 | Loss 0.9884\n",
      "Epoch 15 | Batch 1500 | Accuracy 0.4950 | Loss 0.9844\n",
      "Epoch 15 | Batch 1550 | Accuracy 0.4956 | Loss 0.9809\n",
      "Epoch 15 | Batch 1600 | Accuracy 0.4963 | Loss 0.9777\n",
      "Epoch 15 | Batch 1650 | Accuracy 0.4970 | Loss 0.9742\n",
      "Epoch 15 | Batch 1700 | Accuracy 0.4975 | Loss 0.9702\n",
      "Epoch 15 | Batch 1750 | Accuracy 0.4980 | Loss 0.9668\n",
      "Epoch 15 | Batch 1800 | Accuracy 0.4986 | Loss 0.9633\n",
      "Epoch 15 | Batch 1850 | Accuracy 0.4990 | Loss 0.9609\n",
      "Epoch 15 | Batch 1900 | Accuracy 0.4995 | Loss 0.9581\n",
      "Epoch 15 | Batch 1950 | Accuracy 0.5000 | Loss 0.9553\n",
      "Epoch 15 | Batch 2000 | Accuracy 0.5007 | Loss 0.9530\n",
      "Epoch 15 | Batch 2050 | Accuracy 0.5012 | Loss 0.9502\n",
      "Epoch 15 | Batch 2100 | Accuracy 0.5018 | Loss 0.9477\n",
      "Epoch 15 | Batch 2150 | Accuracy 0.5024 | Loss 0.9458\n",
      "Epoch 15 | Batch 2200 | Accuracy 0.5029 | Loss 0.9439\n",
      "Epoch 15 | Batch 2250 | Accuracy 0.5034 | Loss 0.9421\n",
      "Epoch 15 | Batch 2300 | Accuracy 0.5035 | Loss 0.9421\n",
      "Epoch 15 | Batch 2350 | Accuracy 0.5036 | Loss 0.9426\n",
      "Epoch 15 | Batch 2400 | Accuracy 0.5037 | Loss 0.9435\n",
      "Epoch 15 | Batch 2450 | Accuracy 0.5035 | Loss 0.9451\n",
      "Epoch 15 | Batch 2500 | Accuracy 0.5033 | Loss 0.9479\n",
      "Epoch 15 | Batch 2550 | Accuracy 0.5032 | Loss 0.9502\n",
      "Epoch 15 | Batch 2600 | Accuracy 0.5029 | Loss 0.9528\n",
      "Epoch 15 | Batch 2650 | Accuracy 0.5027 | Loss 0.9556\n",
      "Epoch 15 | Batch 2700 | Accuracy 0.5026 | Loss 0.9586\n",
      "Epoch 15 | Batch 2750 | Accuracy 0.5023 | Loss 0.9615\n",
      "Epoch 15 | Batch 2800 | Accuracy 0.5021 | Loss 0.9644\n",
      "Epoch 15 | Batch 2850 | Accuracy 0.5019 | Loss 0.9668\n",
      "Epoch 15 | Batch 2900 | Accuracy 0.5016 | Loss 0.9695\n",
      "Epoch 15 | Batch 2950 | Accuracy 0.5013 | Loss 0.9720\n",
      "Epoch 15 | Batch 3000 | Accuracy 0.5010 | Loss 0.9747\n",
      "Epoch 15 | Batch 3050 | Accuracy 0.5008 | Loss 0.9767\n",
      "Epoch 15 | Batch 3100 | Accuracy 0.5006 | Loss 0.9790\n",
      "Epoch 15 | Batch 3150 | Accuracy 0.5003 | Loss 0.9812\n",
      "Epoch 15 | Batch 3200 | Accuracy 0.5001 | Loss 0.9832\n",
      "Epoch 15 | Batch 3250 | Accuracy 0.4998 | Loss 0.9852\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 14: 331.6067864894867 secs \n",
      "\n",
      "Epoch 16 of 20\n",
      "Epoch 16 | Batch 0 | Accuracy 0.4821 | Loss 1.1242\n",
      "Epoch 16 | Batch 50 | Accuracy 0.4879 | Loss 1.1283\n",
      "Epoch 16 | Batch 100 | Accuracy 0.4871 | Loss 1.1217\n",
      "Epoch 16 | Batch 150 | Accuracy 0.4863 | Loss 1.1194\n",
      "Epoch 16 | Batch 200 | Accuracy 0.4861 | Loss 1.1114\n",
      "Epoch 16 | Batch 250 | Accuracy 0.4867 | Loss 1.1117\n",
      "Epoch 16 | Batch 300 | Accuracy 0.4874 | Loss 1.1081\n",
      "Epoch 16 | Batch 350 | Accuracy 0.4884 | Loss 1.0995\n",
      "Epoch 16 | Batch 400 | Accuracy 0.4892 | Loss 1.0917\n",
      "Epoch 16 | Batch 450 | Accuracy 0.4896 | Loss 1.0851\n",
      "Epoch 16 | Batch 500 | Accuracy 0.4899 | Loss 1.0786\n",
      "Epoch 16 | Batch 550 | Accuracy 0.4903 | Loss 1.0745\n",
      "Epoch 16 | Batch 600 | Accuracy 0.4909 | Loss 1.0709\n",
      "Epoch 16 | Batch 650 | Accuracy 0.4911 | Loss 1.0658\n",
      "Epoch 16 | Batch 700 | Accuracy 0.4908 | Loss 1.0602\n",
      "Epoch 16 | Batch 750 | Accuracy 0.4910 | Loss 1.0547\n",
      "Epoch 16 | Batch 800 | Accuracy 0.4911 | Loss 1.0515\n",
      "Epoch 16 | Batch 850 | Accuracy 0.4914 | Loss 1.0464\n",
      "Epoch 16 | Batch 900 | Accuracy 0.4915 | Loss 1.0431\n",
      "Epoch 16 | Batch 950 | Accuracy 0.4917 | Loss 1.0389\n",
      "Epoch 16 | Batch 1000 | Accuracy 0.4918 | Loss 1.0342\n",
      "Epoch 16 | Batch 1050 | Accuracy 0.4923 | Loss 1.0271\n",
      "Epoch 16 | Batch 1100 | Accuracy 0.4926 | Loss 1.0192\n",
      "Epoch 16 | Batch 1150 | Accuracy 0.4929 | Loss 1.0126\n",
      "Epoch 16 | Batch 1200 | Accuracy 0.4932 | Loss 1.0063\n",
      "Epoch 16 | Batch 1250 | Accuracy 0.4935 | Loss 1.0002\n",
      "Epoch 16 | Batch 1300 | Accuracy 0.4939 | Loss 0.9953\n",
      "Epoch 16 | Batch 1350 | Accuracy 0.4946 | Loss 0.9899\n",
      "Epoch 16 | Batch 1400 | Accuracy 0.4951 | Loss 0.9853\n",
      "Epoch 16 | Batch 1450 | Accuracy 0.4957 | Loss 0.9819\n",
      "Epoch 16 | Batch 1500 | Accuracy 0.4962 | Loss 0.9778\n",
      "Epoch 16 | Batch 1550 | Accuracy 0.4968 | Loss 0.9736\n",
      "Epoch 16 | Batch 1600 | Accuracy 0.4973 | Loss 0.9704\n",
      "Epoch 16 | Batch 1650 | Accuracy 0.4980 | Loss 0.9667\n",
      "Epoch 16 | Batch 1700 | Accuracy 0.4986 | Loss 0.9628\n",
      "Epoch 16 | Batch 1750 | Accuracy 0.4991 | Loss 0.9591\n",
      "Epoch 16 | Batch 1800 | Accuracy 0.4998 | Loss 0.9561\n",
      "Epoch 16 | Batch 1850 | Accuracy 0.5004 | Loss 0.9537\n",
      "Epoch 16 | Batch 1900 | Accuracy 0.5009 | Loss 0.9507\n",
      "Epoch 16 | Batch 1950 | Accuracy 0.5015 | Loss 0.9483\n",
      "Epoch 16 | Batch 2000 | Accuracy 0.5020 | Loss 0.9460\n",
      "Epoch 16 | Batch 2050 | Accuracy 0.5027 | Loss 0.9437\n",
      "Epoch 16 | Batch 2100 | Accuracy 0.5034 | Loss 0.9418\n",
      "Epoch 16 | Batch 2150 | Accuracy 0.5039 | Loss 0.9395\n",
      "Epoch 16 | Batch 2200 | Accuracy 0.5044 | Loss 0.9370\n",
      "Epoch 16 | Batch 2250 | Accuracy 0.5049 | Loss 0.9356\n",
      "Epoch 16 | Batch 2300 | Accuracy 0.5051 | Loss 0.9356\n",
      "Epoch 16 | Batch 2350 | Accuracy 0.5052 | Loss 0.9356\n",
      "Epoch 16 | Batch 2400 | Accuracy 0.5051 | Loss 0.9367\n",
      "Epoch 16 | Batch 2450 | Accuracy 0.5051 | Loss 0.9387\n",
      "Epoch 16 | Batch 2500 | Accuracy 0.5050 | Loss 0.9408\n",
      "Epoch 16 | Batch 2550 | Accuracy 0.5048 | Loss 0.9436\n",
      "Epoch 16 | Batch 2600 | Accuracy 0.5046 | Loss 0.9463\n",
      "Epoch 16 | Batch 2650 | Accuracy 0.5043 | Loss 0.9483\n",
      "Epoch 16 | Batch 2700 | Accuracy 0.5042 | Loss 0.9511\n",
      "Epoch 16 | Batch 2750 | Accuracy 0.5040 | Loss 0.9536\n",
      "Epoch 16 | Batch 2800 | Accuracy 0.5038 | Loss 0.9564\n",
      "Epoch 16 | Batch 2850 | Accuracy 0.5035 | Loss 0.9592\n",
      "Epoch 16 | Batch 2900 | Accuracy 0.5032 | Loss 0.9615\n",
      "Epoch 16 | Batch 2950 | Accuracy 0.5029 | Loss 0.9641\n",
      "Epoch 16 | Batch 3000 | Accuracy 0.5026 | Loss 0.9667\n",
      "Epoch 16 | Batch 3050 | Accuracy 0.5023 | Loss 0.9692\n",
      "Epoch 16 | Batch 3100 | Accuracy 0.5021 | Loss 0.9715\n",
      "Epoch 16 | Batch 3150 | Accuracy 0.5018 | Loss 0.9735\n",
      "Epoch 16 | Batch 3200 | Accuracy 0.5016 | Loss 0.9755\n",
      "Epoch 16 | Batch 3250 | Accuracy 0.5013 | Loss 0.9777\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 15: 339.08144330978394 secs \n",
      "\n",
      "Epoch 17 of 20\n",
      "Epoch 17 | Batch 0 | Accuracy 0.5022 | Loss 0.9996\n",
      "Epoch 17 | Batch 50 | Accuracy 0.4871 | Loss 1.1390\n",
      "Epoch 17 | Batch 100 | Accuracy 0.4882 | Loss 1.1222\n",
      "Epoch 17 | Batch 150 | Accuracy 0.4891 | Loss 1.1136\n",
      "Epoch 17 | Batch 200 | Accuracy 0.4893 | Loss 1.1104\n",
      "Epoch 17 | Batch 250 | Accuracy 0.4894 | Loss 1.1079\n",
      "Epoch 17 | Batch 300 | Accuracy 0.4895 | Loss 1.1031\n",
      "Epoch 17 | Batch 350 | Accuracy 0.4895 | Loss 1.0971\n",
      "Epoch 17 | Batch 400 | Accuracy 0.4889 | Loss 1.0892\n",
      "Epoch 17 | Batch 450 | Accuracy 0.4888 | Loss 1.0834\n",
      "Epoch 17 | Batch 500 | Accuracy 0.4892 | Loss 1.0773\n",
      "Epoch 17 | Batch 550 | Accuracy 0.4892 | Loss 1.0694\n",
      "Epoch 17 | Batch 600 | Accuracy 0.4900 | Loss 1.0650\n",
      "Epoch 17 | Batch 650 | Accuracy 0.4908 | Loss 1.0604\n",
      "Epoch 17 | Batch 700 | Accuracy 0.4915 | Loss 1.0551\n",
      "Epoch 17 | Batch 750 | Accuracy 0.4919 | Loss 1.0500\n",
      "Epoch 17 | Batch 800 | Accuracy 0.4922 | Loss 1.0443\n",
      "Epoch 17 | Batch 850 | Accuracy 0.4925 | Loss 1.0390\n",
      "Epoch 17 | Batch 900 | Accuracy 0.4926 | Loss 1.0350\n",
      "Epoch 17 | Batch 950 | Accuracy 0.4931 | Loss 1.0302\n",
      "Epoch 17 | Batch 1000 | Accuracy 0.4934 | Loss 1.0262\n",
      "Epoch 17 | Batch 1050 | Accuracy 0.4937 | Loss 1.0202\n",
      "Epoch 17 | Batch 1100 | Accuracy 0.4939 | Loss 1.0145\n",
      "Epoch 17 | Batch 1150 | Accuracy 0.4943 | Loss 1.0079\n",
      "Epoch 17 | Batch 1200 | Accuracy 0.4945 | Loss 1.0020\n",
      "Epoch 17 | Batch 1250 | Accuracy 0.4947 | Loss 0.9961\n",
      "Epoch 17 | Batch 1300 | Accuracy 0.4951 | Loss 0.9904\n",
      "Epoch 17 | Batch 1350 | Accuracy 0.4958 | Loss 0.9849\n",
      "Epoch 17 | Batch 1400 | Accuracy 0.4964 | Loss 0.9805\n",
      "Epoch 17 | Batch 1450 | Accuracy 0.4968 | Loss 0.9759\n",
      "Epoch 17 | Batch 1500 | Accuracy 0.4974 | Loss 0.9713\n",
      "Epoch 17 | Batch 1550 | Accuracy 0.4980 | Loss 0.9680\n",
      "Epoch 17 | Batch 1600 | Accuracy 0.4985 | Loss 0.9645\n",
      "Epoch 17 | Batch 1650 | Accuracy 0.4991 | Loss 0.9597\n",
      "Epoch 17 | Batch 1700 | Accuracy 0.4996 | Loss 0.9560\n",
      "Epoch 17 | Batch 1750 | Accuracy 0.5004 | Loss 0.9520\n",
      "Epoch 17 | Batch 1800 | Accuracy 0.5008 | Loss 0.9486\n",
      "Epoch 17 | Batch 1850 | Accuracy 0.5013 | Loss 0.9460\n",
      "Epoch 17 | Batch 1900 | Accuracy 0.5019 | Loss 0.9434\n",
      "Epoch 17 | Batch 1950 | Accuracy 0.5025 | Loss 0.9407\n",
      "Epoch 17 | Batch 2000 | Accuracy 0.5032 | Loss 0.9380\n",
      "Epoch 17 | Batch 2050 | Accuracy 0.5039 | Loss 0.9358\n",
      "Epoch 17 | Batch 2100 | Accuracy 0.5044 | Loss 0.9338\n",
      "Epoch 17 | Batch 2150 | Accuracy 0.5050 | Loss 0.9319\n",
      "Epoch 17 | Batch 2200 | Accuracy 0.5055 | Loss 0.9306\n",
      "Epoch 17 | Batch 2250 | Accuracy 0.5059 | Loss 0.9291\n",
      "Epoch 17 | Batch 2300 | Accuracy 0.5062 | Loss 0.9285\n",
      "Epoch 17 | Batch 2350 | Accuracy 0.5063 | Loss 0.9292\n",
      "Epoch 17 | Batch 2400 | Accuracy 0.5063 | Loss 0.9301\n",
      "Epoch 17 | Batch 2450 | Accuracy 0.5062 | Loss 0.9320\n",
      "Epoch 17 | Batch 2500 | Accuracy 0.5060 | Loss 0.9341\n",
      "Epoch 17 | Batch 2550 | Accuracy 0.5058 | Loss 0.9367\n",
      "Epoch 17 | Batch 2600 | Accuracy 0.5056 | Loss 0.9394\n",
      "Epoch 17 | Batch 2650 | Accuracy 0.5055 | Loss 0.9418\n",
      "Epoch 17 | Batch 2700 | Accuracy 0.5053 | Loss 0.9446\n",
      "Epoch 17 | Batch 2750 | Accuracy 0.5050 | Loss 0.9478\n",
      "Epoch 17 | Batch 2800 | Accuracy 0.5048 | Loss 0.9504\n",
      "Epoch 17 | Batch 2850 | Accuracy 0.5045 | Loss 0.9535\n",
      "Epoch 17 | Batch 2900 | Accuracy 0.5043 | Loss 0.9560\n",
      "Epoch 17 | Batch 2950 | Accuracy 0.5040 | Loss 0.9583\n",
      "Epoch 17 | Batch 3000 | Accuracy 0.5037 | Loss 0.9609\n",
      "Epoch 17 | Batch 3050 | Accuracy 0.5033 | Loss 0.9630\n",
      "Epoch 17 | Batch 3100 | Accuracy 0.5031 | Loss 0.9654\n",
      "Epoch 17 | Batch 3150 | Accuracy 0.5029 | Loss 0.9674\n",
      "Epoch 17 | Batch 3200 | Accuracy 0.5026 | Loss 0.9695\n",
      "Epoch 17 | Batch 3250 | Accuracy 0.5024 | Loss 0.9711\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 16: 333.36409091949463 secs \n",
      "\n",
      "Epoch 18 of 20\n",
      "Epoch 18 | Batch 0 | Accuracy 0.4989 | Loss 1.3613\n",
      "Epoch 18 | Batch 50 | Accuracy 0.4891 | Loss 1.1113\n",
      "Epoch 18 | Batch 100 | Accuracy 0.4902 | Loss 1.1147\n",
      "Epoch 18 | Batch 150 | Accuracy 0.4889 | Loss 1.1155\n",
      "Epoch 18 | Batch 200 | Accuracy 0.4887 | Loss 1.1118\n",
      "Epoch 18 | Batch 250 | Accuracy 0.4903 | Loss 1.1059\n",
      "Epoch 18 | Batch 300 | Accuracy 0.4911 | Loss 1.0976\n",
      "Epoch 18 | Batch 350 | Accuracy 0.4910 | Loss 1.0900\n",
      "Epoch 18 | Batch 400 | Accuracy 0.4909 | Loss 1.0835\n",
      "Epoch 18 | Batch 450 | Accuracy 0.4911 | Loss 1.0740\n",
      "Epoch 18 | Batch 500 | Accuracy 0.4916 | Loss 1.0655\n",
      "Epoch 18 | Batch 550 | Accuracy 0.4920 | Loss 1.0594\n",
      "Epoch 18 | Batch 600 | Accuracy 0.4925 | Loss 1.0555\n",
      "Epoch 18 | Batch 650 | Accuracy 0.4923 | Loss 1.0486\n",
      "Epoch 18 | Batch 700 | Accuracy 0.4923 | Loss 1.0444\n",
      "Epoch 18 | Batch 750 | Accuracy 0.4924 | Loss 1.0398\n",
      "Epoch 18 | Batch 800 | Accuracy 0.4929 | Loss 1.0365\n",
      "Epoch 18 | Batch 850 | Accuracy 0.4932 | Loss 1.0326\n",
      "Epoch 18 | Batch 900 | Accuracy 0.4936 | Loss 1.0293\n",
      "Epoch 18 | Batch 950 | Accuracy 0.4941 | Loss 1.0237\n",
      "Epoch 18 | Batch 1000 | Accuracy 0.4943 | Loss 1.0198\n",
      "Epoch 18 | Batch 1050 | Accuracy 0.4944 | Loss 1.0156\n",
      "Epoch 18 | Batch 1100 | Accuracy 0.4947 | Loss 1.0076\n",
      "Epoch 18 | Batch 1150 | Accuracy 0.4950 | Loss 1.0009\n",
      "Epoch 18 | Batch 1200 | Accuracy 0.4953 | Loss 0.9944\n",
      "Epoch 18 | Batch 1250 | Accuracy 0.4958 | Loss 0.9886\n",
      "Epoch 18 | Batch 1300 | Accuracy 0.4963 | Loss 0.9833\n",
      "Epoch 18 | Batch 1350 | Accuracy 0.4968 | Loss 0.9776\n",
      "Epoch 18 | Batch 1400 | Accuracy 0.4972 | Loss 0.9721\n",
      "Epoch 18 | Batch 1450 | Accuracy 0.4978 | Loss 0.9676\n",
      "Epoch 18 | Batch 1500 | Accuracy 0.4983 | Loss 0.9633\n",
      "Epoch 18 | Batch 1550 | Accuracy 0.4990 | Loss 0.9598\n",
      "Epoch 18 | Batch 1600 | Accuracy 0.4996 | Loss 0.9561\n",
      "Epoch 18 | Batch 1650 | Accuracy 0.5002 | Loss 0.9526\n",
      "Epoch 18 | Batch 1700 | Accuracy 0.5008 | Loss 0.9489\n",
      "Epoch 18 | Batch 1750 | Accuracy 0.5015 | Loss 0.9454\n",
      "Epoch 18 | Batch 1800 | Accuracy 0.5019 | Loss 0.9426\n",
      "Epoch 18 | Batch 1850 | Accuracy 0.5024 | Loss 0.9398\n",
      "Epoch 18 | Batch 1900 | Accuracy 0.5030 | Loss 0.9379\n",
      "Epoch 18 | Batch 1950 | Accuracy 0.5035 | Loss 0.9354\n",
      "Epoch 18 | Batch 2000 | Accuracy 0.5041 | Loss 0.9335\n",
      "Epoch 18 | Batch 2050 | Accuracy 0.5048 | Loss 0.9310\n",
      "Epoch 18 | Batch 2100 | Accuracy 0.5053 | Loss 0.9285\n",
      "Epoch 18 | Batch 2150 | Accuracy 0.5058 | Loss 0.9265\n",
      "Epoch 18 | Batch 2200 | Accuracy 0.5062 | Loss 0.9245\n",
      "Epoch 18 | Batch 2250 | Accuracy 0.5068 | Loss 0.9225\n",
      "Epoch 18 | Batch 2300 | Accuracy 0.5070 | Loss 0.9225\n",
      "Epoch 18 | Batch 2350 | Accuracy 0.5073 | Loss 0.9225\n",
      "Epoch 18 | Batch 2400 | Accuracy 0.5071 | Loss 0.9239\n",
      "Epoch 18 | Batch 2450 | Accuracy 0.5070 | Loss 0.9259\n",
      "Epoch 18 | Batch 2500 | Accuracy 0.5069 | Loss 0.9281\n",
      "Epoch 18 | Batch 2550 | Accuracy 0.5067 | Loss 0.9306\n",
      "Epoch 18 | Batch 2600 | Accuracy 0.5065 | Loss 0.9334\n",
      "Epoch 18 | Batch 2650 | Accuracy 0.5064 | Loss 0.9361\n",
      "Epoch 18 | Batch 2700 | Accuracy 0.5062 | Loss 0.9387\n",
      "Epoch 18 | Batch 2750 | Accuracy 0.5060 | Loss 0.9407\n",
      "Epoch 18 | Batch 2800 | Accuracy 0.5058 | Loss 0.9432\n",
      "Epoch 18 | Batch 2850 | Accuracy 0.5057 | Loss 0.9455\n",
      "Epoch 18 | Batch 2900 | Accuracy 0.5054 | Loss 0.9483\n",
      "Epoch 18 | Batch 2950 | Accuracy 0.5051 | Loss 0.9504\n",
      "Epoch 18 | Batch 3000 | Accuracy 0.5047 | Loss 0.9530\n",
      "Epoch 18 | Batch 3050 | Accuracy 0.5044 | Loss 0.9553\n",
      "Epoch 18 | Batch 3100 | Accuracy 0.5040 | Loss 0.9580\n",
      "Epoch 18 | Batch 3150 | Accuracy 0.5038 | Loss 0.9604\n",
      "Epoch 18 | Batch 3200 | Accuracy 0.5036 | Loss 0.9625\n",
      "Epoch 18 | Batch 3250 | Accuracy 0.5035 | Loss 0.9647\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 17: 333.7710437774658 secs \n",
      "\n",
      "Epoch 19 of 20\n",
      "Epoch 19 | Batch 0 | Accuracy 0.4944 | Loss 1.0662\n",
      "Epoch 19 | Batch 50 | Accuracy 0.4915 | Loss 1.1282\n",
      "Epoch 19 | Batch 100 | Accuracy 0.4931 | Loss 1.1236\n",
      "Epoch 19 | Batch 150 | Accuracy 0.4914 | Loss 1.1115\n",
      "Epoch 19 | Batch 200 | Accuracy 0.4924 | Loss 1.1049\n",
      "Epoch 19 | Batch 250 | Accuracy 0.4921 | Loss 1.1013\n",
      "Epoch 19 | Batch 300 | Accuracy 0.4926 | Loss 1.0927\n",
      "Epoch 19 | Batch 350 | Accuracy 0.4922 | Loss 1.0839\n",
      "Epoch 19 | Batch 400 | Accuracy 0.4925 | Loss 1.0752\n",
      "Epoch 19 | Batch 450 | Accuracy 0.4926 | Loss 1.0694\n",
      "Epoch 19 | Batch 500 | Accuracy 0.4929 | Loss 1.0613\n",
      "Epoch 19 | Batch 550 | Accuracy 0.4929 | Loss 1.0547\n",
      "Epoch 19 | Batch 600 | Accuracy 0.4930 | Loss 1.0499\n",
      "Epoch 19 | Batch 650 | Accuracy 0.4935 | Loss 1.0447\n",
      "Epoch 19 | Batch 700 | Accuracy 0.4939 | Loss 1.0390\n",
      "Epoch 19 | Batch 750 | Accuracy 0.4944 | Loss 1.0343\n",
      "Epoch 19 | Batch 800 | Accuracy 0.4947 | Loss 1.0290\n",
      "Epoch 19 | Batch 850 | Accuracy 0.4949 | Loss 1.0262\n",
      "Epoch 19 | Batch 900 | Accuracy 0.4951 | Loss 1.0219\n",
      "Epoch 19 | Batch 950 | Accuracy 0.4949 | Loss 1.0184\n",
      "Epoch 19 | Batch 1000 | Accuracy 0.4952 | Loss 1.0137\n",
      "Epoch 19 | Batch 1050 | Accuracy 0.4954 | Loss 1.0086\n",
      "Epoch 19 | Batch 1100 | Accuracy 0.4955 | Loss 1.0023\n",
      "Epoch 19 | Batch 1150 | Accuracy 0.4959 | Loss 0.9954\n",
      "Epoch 19 | Batch 1200 | Accuracy 0.4960 | Loss 0.9890\n",
      "Epoch 19 | Batch 1250 | Accuracy 0.4965 | Loss 0.9833\n",
      "Epoch 19 | Batch 1300 | Accuracy 0.4970 | Loss 0.9773\n",
      "Epoch 19 | Batch 1350 | Accuracy 0.4977 | Loss 0.9723\n",
      "Epoch 19 | Batch 1400 | Accuracy 0.4983 | Loss 0.9675\n",
      "Epoch 19 | Batch 1450 | Accuracy 0.4991 | Loss 0.9628\n",
      "Epoch 19 | Batch 1500 | Accuracy 0.4995 | Loss 0.9588\n",
      "Epoch 19 | Batch 1550 | Accuracy 0.5000 | Loss 0.9549\n",
      "Epoch 19 | Batch 1600 | Accuracy 0.5005 | Loss 0.9516\n",
      "Epoch 19 | Batch 1650 | Accuracy 0.5010 | Loss 0.9477\n",
      "Epoch 19 | Batch 1700 | Accuracy 0.5015 | Loss 0.9439\n",
      "Epoch 19 | Batch 1750 | Accuracy 0.5022 | Loss 0.9407\n",
      "Epoch 19 | Batch 1800 | Accuracy 0.5027 | Loss 0.9376\n",
      "Epoch 19 | Batch 1850 | Accuracy 0.5032 | Loss 0.9352\n",
      "Epoch 19 | Batch 1900 | Accuracy 0.5038 | Loss 0.9330\n",
      "Epoch 19 | Batch 1950 | Accuracy 0.5043 | Loss 0.9299\n",
      "Epoch 19 | Batch 2000 | Accuracy 0.5048 | Loss 0.9277\n",
      "Epoch 19 | Batch 2050 | Accuracy 0.5054 | Loss 0.9253\n",
      "Epoch 19 | Batch 2100 | Accuracy 0.5061 | Loss 0.9230\n",
      "Epoch 19 | Batch 2150 | Accuracy 0.5065 | Loss 0.9208\n",
      "Epoch 19 | Batch 2200 | Accuracy 0.5070 | Loss 0.9186\n",
      "Epoch 19 | Batch 2250 | Accuracy 0.5075 | Loss 0.9175\n",
      "Epoch 19 | Batch 2300 | Accuracy 0.5077 | Loss 0.9166\n",
      "Epoch 19 | Batch 2350 | Accuracy 0.5079 | Loss 0.9171\n",
      "Epoch 19 | Batch 2400 | Accuracy 0.5080 | Loss 0.9183\n",
      "Epoch 19 | Batch 2450 | Accuracy 0.5079 | Loss 0.9199\n",
      "Epoch 19 | Batch 2500 | Accuracy 0.5077 | Loss 0.9223\n",
      "Epoch 19 | Batch 2550 | Accuracy 0.5075 | Loss 0.9249\n",
      "Epoch 19 | Batch 2600 | Accuracy 0.5074 | Loss 0.9277\n",
      "Epoch 19 | Batch 2650 | Accuracy 0.5073 | Loss 0.9299\n",
      "Epoch 19 | Batch 2700 | Accuracy 0.5070 | Loss 0.9323\n",
      "Epoch 19 | Batch 2750 | Accuracy 0.5068 | Loss 0.9353\n",
      "Epoch 19 | Batch 2800 | Accuracy 0.5065 | Loss 0.9383\n",
      "Epoch 19 | Batch 2850 | Accuracy 0.5063 | Loss 0.9407\n",
      "Epoch 19 | Batch 2900 | Accuracy 0.5060 | Loss 0.9440\n",
      "Epoch 19 | Batch 2950 | Accuracy 0.5057 | Loss 0.9470\n",
      "Epoch 19 | Batch 3000 | Accuracy 0.5053 | Loss 0.9492\n",
      "Epoch 19 | Batch 3050 | Accuracy 0.5051 | Loss 0.9511\n",
      "Epoch 19 | Batch 3100 | Accuracy 0.5048 | Loss 0.9531\n",
      "Epoch 19 | Batch 3150 | Accuracy 0.5046 | Loss 0.9553\n",
      "Epoch 19 | Batch 3200 | Accuracy 0.5043 | Loss 0.9572\n",
      "Epoch 19 | Batch 3250 | Accuracy 0.5041 | Loss 0.9594\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 18: 328.32604598999023 secs \n",
      "\n",
      "Epoch 20 of 20\n",
      "Epoch 20 | Batch 0 | Accuracy 0.4933 | Loss 1.4705\n",
      "Epoch 20 | Batch 50 | Accuracy 0.4928 | Loss 1.0934\n",
      "Epoch 20 | Batch 100 | Accuracy 0.4940 | Loss 1.0839\n",
      "Epoch 20 | Batch 150 | Accuracy 0.4927 | Loss 1.0809\n",
      "Epoch 20 | Batch 200 | Accuracy 0.4923 | Loss 1.0840\n",
      "Epoch 20 | Batch 250 | Accuracy 0.4925 | Loss 1.0854\n",
      "Epoch 20 | Batch 300 | Accuracy 0.4937 | Loss 1.0809\n",
      "Epoch 20 | Batch 350 | Accuracy 0.4932 | Loss 1.0736\n",
      "Epoch 20 | Batch 400 | Accuracy 0.4938 | Loss 1.0663\n",
      "Epoch 20 | Batch 450 | Accuracy 0.4940 | Loss 1.0577\n",
      "Epoch 20 | Batch 500 | Accuracy 0.4946 | Loss 1.0537\n",
      "Epoch 20 | Batch 550 | Accuracy 0.4945 | Loss 1.0500\n",
      "Epoch 20 | Batch 600 | Accuracy 0.4947 | Loss 1.0460\n",
      "Epoch 20 | Batch 650 | Accuracy 0.4951 | Loss 1.0400\n",
      "Epoch 20 | Batch 700 | Accuracy 0.4951 | Loss 1.0351\n",
      "Epoch 20 | Batch 750 | Accuracy 0.4956 | Loss 1.0297\n",
      "Epoch 20 | Batch 800 | Accuracy 0.4956 | Loss 1.0266\n",
      "Epoch 20 | Batch 850 | Accuracy 0.4962 | Loss 1.0224\n",
      "Epoch 20 | Batch 900 | Accuracy 0.4962 | Loss 1.0181\n",
      "Epoch 20 | Batch 950 | Accuracy 0.4961 | Loss 1.0138\n",
      "Epoch 20 | Batch 1000 | Accuracy 0.4964 | Loss 1.0084\n",
      "Epoch 20 | Batch 1050 | Accuracy 0.4966 | Loss 1.0020\n",
      "Epoch 20 | Batch 1100 | Accuracy 0.4967 | Loss 0.9946\n",
      "Epoch 20 | Batch 1150 | Accuracy 0.4971 | Loss 0.9885\n",
      "Epoch 20 | Batch 1200 | Accuracy 0.4974 | Loss 0.9835\n",
      "Epoch 20 | Batch 1250 | Accuracy 0.4976 | Loss 0.9781\n",
      "Epoch 20 | Batch 1300 | Accuracy 0.4980 | Loss 0.9716\n",
      "Epoch 20 | Batch 1350 | Accuracy 0.4986 | Loss 0.9664\n",
      "Epoch 20 | Batch 1400 | Accuracy 0.4991 | Loss 0.9614\n",
      "Epoch 20 | Batch 1450 | Accuracy 0.4998 | Loss 0.9568\n",
      "Epoch 20 | Batch 1500 | Accuracy 0.5004 | Loss 0.9527\n",
      "Epoch 20 | Batch 1550 | Accuracy 0.5010 | Loss 0.9493\n",
      "Epoch 20 | Batch 1600 | Accuracy 0.5013 | Loss 0.9449\n",
      "Epoch 20 | Batch 1650 | Accuracy 0.5021 | Loss 0.9412\n",
      "Epoch 20 | Batch 1700 | Accuracy 0.5027 | Loss 0.9376\n",
      "Epoch 20 | Batch 1750 | Accuracy 0.5033 | Loss 0.9343\n",
      "Epoch 20 | Batch 1800 | Accuracy 0.5039 | Loss 0.9310\n",
      "Epoch 20 | Batch 1850 | Accuracy 0.5043 | Loss 0.9283\n",
      "Epoch 20 | Batch 1900 | Accuracy 0.5048 | Loss 0.9257\n",
      "Epoch 20 | Batch 1950 | Accuracy 0.5054 | Loss 0.9230\n",
      "Epoch 20 | Batch 2000 | Accuracy 0.5061 | Loss 0.9204\n",
      "Epoch 20 | Batch 2050 | Accuracy 0.5066 | Loss 0.9182\n",
      "Epoch 20 | Batch 2100 | Accuracy 0.5070 | Loss 0.9162\n",
      "Epoch 20 | Batch 2150 | Accuracy 0.5076 | Loss 0.9143\n",
      "Epoch 20 | Batch 2200 | Accuracy 0.5082 | Loss 0.9125\n",
      "Epoch 20 | Batch 2250 | Accuracy 0.5086 | Loss 0.9107\n",
      "Epoch 20 | Batch 2300 | Accuracy 0.5089 | Loss 0.9103\n",
      "Epoch 20 | Batch 2350 | Accuracy 0.5091 | Loss 0.9109\n",
      "Epoch 20 | Batch 2400 | Accuracy 0.5091 | Loss 0.9117\n",
      "Epoch 20 | Batch 2450 | Accuracy 0.5090 | Loss 0.9139\n",
      "Epoch 20 | Batch 2500 | Accuracy 0.5087 | Loss 0.9161\n",
      "Epoch 20 | Batch 2550 | Accuracy 0.5085 | Loss 0.9191\n",
      "Epoch 20 | Batch 2600 | Accuracy 0.5083 | Loss 0.9219\n",
      "Epoch 20 | Batch 2650 | Accuracy 0.5082 | Loss 0.9246\n",
      "Epoch 20 | Batch 2700 | Accuracy 0.5080 | Loss 0.9274\n",
      "Epoch 20 | Batch 2750 | Accuracy 0.5078 | Loss 0.9301\n",
      "Epoch 20 | Batch 2800 | Accuracy 0.5075 | Loss 0.9328\n",
      "Epoch 20 | Batch 2850 | Accuracy 0.5072 | Loss 0.9356\n",
      "Epoch 20 | Batch 2900 | Accuracy 0.5068 | Loss 0.9385\n",
      "Epoch 20 | Batch 2950 | Accuracy 0.5065 | Loss 0.9410\n",
      "Epoch 20 | Batch 3000 | Accuracy 0.5062 | Loss 0.9434\n",
      "Epoch 20 | Batch 3050 | Accuracy 0.5060 | Loss 0.9459\n",
      "Epoch 20 | Batch 3100 | Accuracy 0.5057 | Loss 0.9479\n",
      "Epoch 20 | Batch 3150 | Accuracy 0.5053 | Loss 0.9496\n",
      "Epoch 20 | Batch 3200 | Accuracy 0.5051 | Loss 0.9517\n",
      "Epoch 20 | Batch 3250 | Accuracy 0.5050 | Loss 0.9537\n",
      "Saving checkpoint...\n",
      "Time taken in epoch 19: 326.0667951107025 secs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {} of {}'.format(epoch + 1, epochs))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # reset states\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "\n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        # shifted right\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = transformer(enc_inputs = enc_inputs, dec_inputs = dec_inputs, training = True)\n",
    "\n",
    "            loss = loss_function(target = dec_outputs_real, pred = pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_acc(dec_outputs_real, pred)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} | Batch {} | Accuracy {:.4f} | Loss {:.4f}'.format(epoch+1, batch, train_acc.result(), train_loss.result()))\n",
    "    \n",
    "    ckpt_manager.save()\n",
    "    print('Saving checkpoint...')\n",
    "    print('Time taken in epoch {}: {} secs \\n'.format(epoch, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cbe59bc8e43c33e263d0ddce44ad7ec5ba3f57d4d2daa17d616eb06d4130f325"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
